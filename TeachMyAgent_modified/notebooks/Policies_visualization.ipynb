{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "This notebook is made to help analysing results produced by TeachMyAgent's experiments. Using this, one can generate videos of policies as shown on our [website](https://developmentalsystems.org/TeachMyAgent/). \n",
    "\n",
    "## How to use this notebook\n",
    "This notebook is broken down into 4 sections:\n",
    "- **Imports**: import needed packages.\n",
    "- **Load Data**: load results produced by experiments and format them (e.g. calculate best seed of each experiment).\n",
    "- **Plot definitions**: define all the plot functions we provide.\n",
    "- **Experiment graphs**: use the previously defined functions to generate the different figures.\n",
    "\n",
    "## Add our paper's results to your plots\n",
    "In order to add the results we provide in our paper to your plots, make sure you have downloaded them:\n",
    "1. Go to the `notebooks` folder\n",
    "2. Make the `download_baselines.sh` script executable: `chmod +x download_baselines.sh`\n",
    "3. Download results: `./download_baselines.sh`\n",
    "> **_WARNING:_**  This will download a zip weighting approximayely 4.5GB. Then, our script will extract the zip file in `TeachMyAgent/data`. Once extracted, results will weight approximately 15GB. \n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import pylab\n",
    "import copy\n",
    "import re\n",
    "from enum import Enum\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "DIV_LINE_WIDTH = 50\n",
    "print(np.__version__)\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import TeachMyAgent.students.test_policy as test_policy\n",
    "from TeachMyAgent.students.run_logs_util import get_run_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_datasets(rootdir, name_filter=None, rename_labels=False):\n",
    "    \"\"\"\n",
    "        Loads results of experiments.\n",
    " \n",
    "        Results to load can be filtered by their name and each experiment can be associated to a label (usually ACL method's name)\n",
    " \n",
    "        :param rootdir: Directory containing experiments to load (do not forget '/' at the end of the path)\n",
    "        :param name_filter: String experiments to load must contain\n",
    "        :param rename_labels: If True, each experiment will be associated to a label (see below). Labels are the names that will appear in plots.\n",
    "        :type rootdir: str\n",
    "        :type name_filter: str (or None)\n",
    "        :type rename_labels: boolean\n",
    "    \"\"\"\n",
    "    _, models_list, _ = next(os.walk(rootdir))\n",
    "    print(models_list)\n",
    "    for dir_name in models_list.copy():\n",
    "        if \"ignore\" in dir_name:\n",
    "            models_list.remove(dir_name)\n",
    "        if name_filter is not None and name_filter not in dir_name:\n",
    "            models_list.remove(dir_name)         \n",
    "        \n",
    "    for i,m_name in enumerate(models_list):           \n",
    "        print(\"extracting data for {}...\".format(m_name))\n",
    "        m_id = m_name\n",
    "        models_saves[m_id] = OrderedDict()\n",
    "        models_saves[m_id]['data'] = get_run_logs(rootdir+m_name, book_keeping_keys=['env_test_rewards'], min_len=0)\n",
    "        print(\"done\")\n",
    "        if m_name not in labels:\n",
    "            if not rename_labels:\n",
    "                labels[m_name] = m_name\n",
    "            else:\n",
    "                ##### MODIFY THIS IF YOU ADD A NEW METHOD #####\n",
    "                if 'ADR' in m_name:\n",
    "                    labels[m_name] = 'ADR'\n",
    "                elif 'ALP-GMM' in m_name:\n",
    "                    labels[m_name] = 'ALP-GMM'\n",
    "                elif 'Random' in m_name:\n",
    "                    labels[m_name] = 'Random'\n",
    "                elif 'Covar-GMM' in m_name:\n",
    "                    labels[m_name] = 'Covar-GMM'\n",
    "                elif 'RIAC' in m_name:\n",
    "                    labels[m_name] = 'RIAC'\n",
    "                elif 'GoalGAN' in m_name:\n",
    "                    labels[m_name] = 'GoalGAN'\n",
    "                elif 'Self-Paced' in m_name:\n",
    "                    labels[m_name] = 'Self-Paced'\n",
    "                elif 'Setter-Solver' in m_name:\n",
    "                    labels[m_name] = 'Setter-Solver'\n",
    "                elif 'UPPER_BASELINE' in m_name:\n",
    "                    labels[m_name] = 'UPPER_BASELINE'\n",
    "                else:\n",
    "                    labels[m_name] = m_name\n",
    "                ##### MODIFY THIS IF YOU ADD A NEW METHOD #####\n",
    "labels = OrderedDict()\n",
    "models_saves = OrderedDict()\n",
    "\n",
    "##### MODIFY THIS TO POINT TO YOUR DATA FOLDER #####\n",
    "data_folder = \"../TeachMyAgent/data/BENCHMARK/\"\n",
    "##### MODIFY THIS TO POINT TO YOUR DATA FOLDER #####\n",
    "\n",
    "get_datasets(data_folder, rename_labels=True)\n",
    "# get_datasets(data_folder, rename_labels=True, name_filter=\"parkour_RIAC_walker_type_fish\") # You can also add filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute mastered tasks percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute \"% of Mastered tasks\" metric: percentage of test tasks (over a test set of 100 tasks) on which the agent obtained an episodic reward greater than a threshold (230)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mastered_thr = 230\n",
    "for i,(m_id,label) in enumerate(labels.items()):\n",
    "    print(m_id)\n",
    "    runs_data = models_saves[m_id]['data']\n",
    "    #collect raw perfs\n",
    "    print(\"Seeds : \" + str(len(runs_data)))\n",
    "    for r,run in enumerate(runs_data):\n",
    "        models_saves[m_id]['data'][r]['nb_mastered'] = []\n",
    "        models_saves[m_id]['data'][r]['avg_pos_rewards'] = []\n",
    "        models_saves[m_id]['data'][r]['local_rewards'] = []\n",
    "        if 'env_test_rewards' in run:\n",
    "            size_test_set = int(len(run['env_test_rewards'])/len(run['evaluation return']))\n",
    "            for j in range(len(run['evaluation return'])):#max_epoch):\n",
    "                test_data = np.array(run['env_test_rewards'][j*size_test_set:(j+1)*(size_test_set)])\n",
    "                nb_mastered = len(np.where(test_data > mastered_thr)[0])\n",
    "                models_saves[m_id]['data'][r]['nb_mastered'].append((nb_mastered/size_test_set)*100)\n",
    "        else:\n",
    "            print(\"Skipping seed {}\".format(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute best seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get best seed of each experiment. This is then used to analyze test set performances and show curricula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_seed(expe_name, metric=\"evaluation return\"):\n",
    "    \"\"\"\n",
    "        Calculate best seed of an experiment.\n",
    " \n",
    "        :param expe_name: Experiment's name\n",
    "        :param metric: Metric to use to calculate best seed\n",
    "        :type expe_name: str\n",
    "        :type metric: str\n",
    "        :return best seed, its metric value, mean of all seeds, std over seeds\n",
    "    \"\"\"\n",
    "    best_seed = -1\n",
    "    best_seed_value = -1000\n",
    "    runs_data = models_saves[expe_name]['data']\n",
    "    all_values = []\n",
    "    for run in runs_data:\n",
    "        if len(run[metric]) > 0:\n",
    "            data = run[metric][-1]\n",
    "            all_values.append(data)\n",
    "            if data > best_seed_value:\n",
    "                best_seed_value = data\n",
    "                best_seed = run[\"config\"][\"seed\"]\n",
    "        else:\n",
    "            print(\"Skipping seed {}: no data\".format(run[\"config\"][\"seed\"]))\n",
    "    return best_seed, best_seed_value, np.mean(all_values), np.std(all_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_seeds = {}\n",
    "for i,(m_id,label) in enumerate(labels.items()):\n",
    "    best_seed, best_seed_value, mean, std = get_best_seed(m_id, metric=\"nb_mastered\")\n",
    "    best_seeds[m_id] = best_seed\n",
    "    print(\"Expe {0} : {1} ({2}) - Mean: {3} ({4})\".format(m_id, best_seed, best_seed_value, mean, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_args_str(dictionary):\n",
    "    args_str = []\n",
    "    for key in dictionary:\n",
    "        args_str.append(\"--{}\".format(key))\n",
    "        if dictionary[key] is not None:\n",
    "            args_str.append(\"{}\".format(dictionary[key]))\n",
    "\n",
    "    return args_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy_perf(dataset_folder, settings):\n",
    "    \"\"\"\n",
    "        Test best seed of chosen experiments and get the rewards obtained.\n",
    " \n",
    "        :param dataset_folder: Directory containing experiments to load (do not forget '/' at the end of the path)\n",
    "        :param settings: Dictionary defining experiments to load \n",
    "        :return list of rewards\n",
    "    \"\"\"\n",
    "    parser = test_policy.get_parser()\n",
    "    parser.add_argument('--expe_name', type=str)\n",
    "    ep_returns = []\n",
    "    \n",
    "    for setting in settings:\n",
    "        current_expe_best_seed = best_seeds[setting[\"expe_name\"]]\n",
    "        data_path = os.path.join(dataset_folder, setting[\"expe_name\"], setting[\"expe_name\"] + \"_s\" + str(current_expe_best_seed))\n",
    "        setting[\"fpath\"] = data_path\n",
    "        setting[\"record\"] = False\n",
    "        setting[\"norender\"] = None\n",
    "    \n",
    "        args_str = dict_to_args_str(setting)\n",
    "\n",
    "        args = parser.parse_args(args_str)\n",
    "        ep_returns.append(test_policy.main(args))\n",
    "    return ep_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_policy(dataset_folder, settings):\n",
    "    \"\"\"\n",
    "        Record the policy associated to the best seed of chosen experiments.\n",
    " \n",
    "        :param dataset_folder: Directory containing experiments to load (do not forget '/' at the end of the path)\n",
    "        :param settings: Dictionary defining experiments to load \n",
    "    \"\"\"\n",
    "    parser = test_policy.get_parser()\n",
    "    parser.add_argument('--expe_name', type=str)\n",
    "    \n",
    "    for setting in settings:\n",
    "        current_expe_best_seed = best_seeds[setting[\"expe_name\"]]\n",
    "        data_path = os.path.join(dataset_folder, setting[\"expe_name\"], setting[\"expe_name\"] + \"_s\" + str(current_expe_best_seed))\n",
    "        setting[\"fpath\"] = data_path\n",
    "        setting[\"record\"] = True\n",
    "        setting[\"recording_path\"] = os.path.join(setting[\"recording_path\"], setting[\"expe_name\"] + \"_s\" + str(current_expe_best_seed))\n",
    "    \n",
    "        args_str = dict_to_args_str(setting)\n",
    "\n",
    "        args = parser.parse_args(args_str)\n",
    "        test_policy.main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the settings below to load the best seed on one of your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_policy(data_folder, settings=[\n",
    "    {\n",
    "        \"env\": \"parametric-continuous-parkour-v0\",\n",
    "        \"embodiment\": \"fish\",\n",
    "        \"bests\": True, # Whether the results on test set should be ordered by performance (best performance first)\n",
    "        \"lidars_type\": \"full\", # Use 'up' for climbers, 'down' for walkers and 'full' for swimmers\n",
    "        \"deterministic\": None, # Leave this to None\n",
    "        \"len\": 2000, # Leave this to 2000\n",
    "        \"expe_name\" : \"04-01_benchmark_parkour_RIAC_walker_type_fish\",\n",
    "        \"episode_ids\": \"0\", # Nth best (or worse if bests=False) tasks to record (-1 means all the episodes). Separate tasks with '/'.\n",
    "        \"recording_path\": \"\" # Path to save the video\n",
    "    },\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = test_policy_perf(data_folder, settings=[\n",
    "    {\n",
    "        \"env\": \"parametric-continuous-parkour-v0\",\n",
    "        \"embodiment\": \"climbing_profile_chimpanzee\",\n",
    "        \"bests\": True, # Whether the results on test set should be ordered by performance (best performance first)\n",
    "        \"lidars_type\": \"up\", # Use 'up' for climbers, 'down' for walkers and 'full' for swimmers\n",
    "        \"deterministic\": None, # Leave this to None\n",
    "        \"len\": 2000, # Leave this to 2000\n",
    "        \"expe_name\" : \"10-08_subset_parkour_climbing_easy_parkour_1_teacher_Random\",\n",
    "        \"fixed_test_set\": 'walking_test_set_v1', # test set to load (remove this if you want to load the test set used during the experiment)\n",
    "        \"episode_ids\": \"0\", # Nth best (or worse if bests=False) tasks to record (-1 means all the episodes). Separate tasks with '/'.\n",
    "        \"recording_path\": \"\" # Path to save the video\n",
    "    },\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
