{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "This notebook is made to help analysing results produced by TeachMyAgent's experiments. Using this, one can analyze book-keeped information from training including performance on tasks for each seed and curriculum monitoring gifs we show on our [website](https://developmentalsystems.org/TeachMyAgent/). \n",
    "\n",
    "## How to use this notebook\n",
    "This notebook is broken down into 4 sections:\n",
    "- **Imports**: import needed packages.\n",
    "- **Load Data**: load results produced by experiments and format them (e.g. calculate best seed of each experiment).\n",
    "- **Plot definitions**: define all the plot functions we provide.\n",
    "- **Experiment graphs**: use the previously defined functions to generate the different figures.\n",
    "\n",
    "## Add our paper's results to your plots\n",
    "In order to add the results we provide in our paper to your plots, make sure you have downloaded them:\n",
    "1. Go to the `notebooks` folder\n",
    "2. Make the `download_baselines.sh` script executable: `chmod +x download_baselines.sh`\n",
    "3. Download results: `./download_baselines.sh`\n",
    "> **_WARNING:_**  This will download a zip weighting approximayely 4.5GB. Then, our script will extract the zip file in `TeachMyAgent/data`. Once extracted, results will weight approximately 15GB. \n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import pylab\n",
    "import copy\n",
    "import re\n",
    "from enum import Enum\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.colorbar as cbar\n",
    "import seaborn as sns\n",
    "import imageio\n",
    "\n",
    "DIV_LINE_WIDTH = 50\n",
    "print(np.__version__)\n",
    "print(sys.executable)\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from TeachMyAgent.run_utils.environment_args_handler import EnvironmentArgsHandler\n",
    "import TeachMyAgent.students.test_policy as test_policy\n",
    "from TeachMyAgent.students.run_logs_util import get_run_logs\n",
    "from TeachMyAgent.teachers.teacher_controller import param_vec_to_param_dict, param_dict_to_param_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_datasets(rootdir, name_filter=None, rename_labels=False):\n",
    "    \"\"\"\n",
    "        Loads results of experiments.\n",
    " \n",
    "        Results to load can be filtered by their name and each experiment can be associated to a label (usually ACL method's name)\n",
    " \n",
    "        :param rootdir: Directory containing experiments to load (do not forget '/' at the end of the path)\n",
    "        :param name_filter: String experiments to load must contain\n",
    "        :param rename_labels: If True, each experiment will be associated to a label (see below). Labels are the names that will appear in plots.\n",
    "        :type rootdir: str\n",
    "        :type name_filter: str (or None)\n",
    "        :type rename_labels: boolean\n",
    "    \"\"\"\n",
    "    _, models_list, _ = next(os.walk(rootdir))\n",
    "    print(models_list)\n",
    "    for dir_name in models_list.copy():\n",
    "        if \"ignore\" in dir_name:\n",
    "            models_list.remove(dir_name)\n",
    "        if name_filter is not None and name_filter not in dir_name:\n",
    "            models_list.remove(dir_name)         \n",
    "        \n",
    "    for i,m_name in enumerate(models_list):           \n",
    "        print(\"extracting data for {}...\".format(m_name))\n",
    "        m_id = m_name\n",
    "        models_saves[m_id] = OrderedDict()\n",
    "        models_saves[m_id]['data'] = get_run_logs(rootdir+m_name, book_keeping_keys='*', min_len=0)\n",
    "        print(\"done\")\n",
    "        if m_name not in labels:\n",
    "            if not rename_labels:\n",
    "                labels[m_name] = m_name\n",
    "            else:\n",
    "                ##### MODIFY THIS IF YOU ADD A NEW METHOD #####\n",
    "                if 'ADR' in m_name:\n",
    "                    labels[m_name] = 'ADR'\n",
    "                elif 'ALP-GMM' in m_name:\n",
    "                    labels[m_name] = 'ALP-GMM'\n",
    "                elif 'Random' in m_name:\n",
    "                    labels[m_name] = 'Random'\n",
    "                elif 'Covar-GMM' in m_name:\n",
    "                    labels[m_name] = 'Covar-GMM'\n",
    "                elif 'RIAC' in m_name:\n",
    "                    labels[m_name] = 'RIAC'\n",
    "                elif 'GoalGAN' in m_name:\n",
    "                    labels[m_name] = 'GoalGAN'\n",
    "                elif 'Self-Paced' in m_name:\n",
    "                    labels[m_name] = 'Self-Paced'\n",
    "                elif 'Setter-Solver' in m_name:\n",
    "                    labels[m_name] = 'Setter-Solver'\n",
    "                elif 'UPPER_BASELINE' in m_name:\n",
    "                    labels[m_name] = 'UPPER_BASELINE'\n",
    "                else:\n",
    "                    labels[m_name] = m_name\n",
    "                ##### MODIFY THIS IF YOU ADD A NEW METHOD #####\n",
    "labels = OrderedDict()\n",
    "models_saves = OrderedDict()\n",
    "\n",
    "##### MODIFY THIS TO POINT TO YOUR DATA FOLDER #####\n",
    "data_folder = \"../TeachMyAgent/data/BENCHMARK/\"\n",
    "##### MODIFY THIS TO POINT TO YOUR DATA FOLDER #####\n",
    "\n",
    "get_datasets(data_folder, rename_labels=True)\n",
    "# get_datasets(data_folder, rename_labels=True, name_filter=\"parkour_RIAC_walker_type_fish\") # You can also add filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute mastered tasks percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute \"% of Mastered tasks\" metric: percentage of test tasks (over a test set of 100 tasks) on which the agent obtained an episodic reward greater than a threshold (230)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mastered_thr = 230\n",
    "for i,(m_id,label) in enumerate(labels.items()):\n",
    "    print(m_id)\n",
    "    runs_data = models_saves[m_id]['data']\n",
    "    #collect raw perfs\n",
    "    print(\"Seeds : \" + str(len(runs_data)))\n",
    "    for r,run in enumerate(runs_data):\n",
    "        models_saves[m_id]['data'][r]['nb_mastered'] = []\n",
    "        models_saves[m_id]['data'][r]['avg_pos_rewards'] = []\n",
    "        models_saves[m_id]['data'][r]['local_rewards'] = []\n",
    "        if 'env_test_rewards' in run:\n",
    "            size_test_set = int(len(run['env_test_rewards'])/len(run['evaluation return']))\n",
    "            for j in range(len(run['evaluation return'])):#max_epoch):\n",
    "                test_data = np.array(run['env_test_rewards'][j*size_test_set:(j+1)*(size_test_set)])\n",
    "                nb_mastered = len(np.where(test_data > mastered_thr)[0])\n",
    "                models_saves[m_id]['data'][r]['nb_mastered'].append((nb_mastered/size_test_set)*100)\n",
    "        else:\n",
    "            print(\"Skipping seed {}\".format(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute best seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get best seed of each experiment. This is then used to analyze test set performances and show curricula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_seed(expe_name, metric=\"evaluation return\"):\n",
    "    \"\"\"\n",
    "        Calculate best seed of an experiment.\n",
    " \n",
    "        :param expe_name: Experiment's name\n",
    "        :param metric: Metric to use to calculate best seed\n",
    "        :type expe_name: str\n",
    "        :type metric: str\n",
    "        :return best seed, its metric value, mean of all seeds, std over seeds\n",
    "    \"\"\"\n",
    "    best_seed = -1\n",
    "    best_seed_value = -1000\n",
    "    runs_data = models_saves[expe_name]['data']\n",
    "    all_values = []\n",
    "    for run in runs_data:\n",
    "        if len(run[metric]) > 0:\n",
    "            data = run[metric][-1]\n",
    "            all_values.append(data)\n",
    "            if data > best_seed_value:\n",
    "                best_seed_value = data\n",
    "                best_seed = run[\"config\"][\"seed\"]\n",
    "        else:\n",
    "            print(\"Skipping seed {}: no data\".format(run[\"config\"][\"seed\"]))\n",
    "    return best_seed, best_seed_value, np.mean(all_values), np.std(all_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_seeds = {}\n",
    "for i,(m_id,label) in enumerate(labels.items()):\n",
    "    best_seed, best_seed_value, mean, std = get_best_seed(m_id, metric=\"nb_mastered\")\n",
    "    best_seeds[m_id] = best_seed\n",
    "    print(\"Expe {0} : {1} ({2}) - Mean: {3} ({4})\".format(m_id, best_seed, best_seed_value, mean, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_args_str(dictionary):\n",
    "    args_str = []\n",
    "    for key in dictionary:\n",
    "        args_str.append(\"--{}\".format(key))\n",
    "        if dictionary[key] is not None:\n",
    "            args_str.append(\"{}\".format(dictionary[key]))\n",
    "\n",
    "    return args_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_values(values):\n",
    "    if isinstance(values, np.ndarray):\n",
    "        for i in range(len(values)):\n",
    "            values[i] = round(values[i], 3)\n",
    "    else:\n",
    "        values = round(values, 3)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_to_str(params_dict, line_width=116):\n",
    "    result = str(params_dict)\n",
    "    nb_splits = max(1, len(result) // line_width)\n",
    "    final_result = \"\"\n",
    "    for i in range(nb_splits):\n",
    "        p1 = result[i*line_width:line_width]\n",
    "        p2 = result[(i+1)*line_width:(i+2)*line_width]\n",
    "        final_result = final_result + p1 + \"\\n\" + p2\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test_tasks_results(env, env_params_list, env_rewards_list, fig_name, nb_env_test_to_check=None):\n",
    "    \"\"\"\n",
    "        Plot test tasks and associated reward obtained.\n",
    " \n",
    "        :param env: An instance of one of the two TeachMyAgent's environments\n",
    "        :param env_params_list: List of tasks (i.e. vector controlling PCG)\n",
    "        :param env_rewards_list: List of associated reward\n",
    "        :param fig_name: Name of the figure\n",
    "        :param nb_env_test_to_check: Plot only the N first tasks (if None plot all tasks)\n",
    "    \"\"\"\n",
    "    nb_env = len(env_params_list) if nb_env_test_to_check is None else nb_env_test_to_check\n",
    "    nb_plots_per_row = 2\n",
    "    nb_rows = math.ceil(nb_env/nb_plots_per_row)\n",
    "    f = plt.figure()\n",
    "    f.set_figwidth(25)\n",
    "    f.set_figheight(6*nb_rows)\n",
    "        \n",
    "    for i in range(nb_env):\n",
    "        fig = plt.subplot(nb_rows, nb_plots_per_row, i+1)\n",
    "        rounded_current_params = {k: round_values(v) for k, v in env_params_list[i].items()}\n",
    "        fig.text(-0.05, 1.03, \n",
    "                 \"Test env nb {0} \\nScore performed: {1} \\nEnv params: {2}\".format(i, env_rewards_list[i], params_to_str(rounded_current_params)), \n",
    "                     ha=\"left\", transform=fig.transAxes)\n",
    "        \n",
    "        env.set_environment(**env_params_list[i])\n",
    "        env.reset()\n",
    "        \n",
    "        plt.imshow(env.render(mode='rgb_array'))\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.savefig('../TeachMyAgent/graphics/{}.png'.format(fig_name), bbox_inches='tight', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_test_sets_analysis(dataset_folder, settings, nb_tasks=-1, test_set=None, ep_returns=None):\n",
    "    \"\"\"\n",
    "        Load results obtained by the best seed of chosen experiments and plot their performance on test set at the end of the experiment.\n",
    " \n",
    "        :param dataset_folder: Directory containing experiments to load (do not forget '/' at the end of the path)\n",
    "        :param settings: Dictionary defining experiments to load \n",
    "        :param nb_tasks: Number of tasks to load per experiment (-1 means all)\n",
    "        :param test_set: Whether another test set than the one used during the experiments must be loaded. If so, specify its name\n",
    "        :param ep_returns: If another test set is used, specify the new list(s) of rewards obtained. You can use `test_policy_perf` of the `Policies_visualization` notebook\n",
    "    \"\"\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "    parser = test_policy.get_parser()\n",
    "    parser.add_argument('--expe_name', type=str)\n",
    "    \n",
    "    i = 0\n",
    "    result = {}\n",
    "    for setting in settings:\n",
    "        current_expe_best_seed = best_seeds[setting[\"expe_name\"]]\n",
    "        data_path = os.path.join(dataset_folder, setting[\"expe_name\"], setting[\"expe_name\"] + \"_s\" + str(current_expe_best_seed))\n",
    "        setting[\"fpath\"] = data_path\n",
    "    \n",
    "        args_str = dict_to_args_str(setting)\n",
    "\n",
    "        args = parser.parse_args(args_str)\n",
    "        env_fn, param_bounds, _, _ = EnvironmentArgsHandler.get_object_from_arguments(args)\n",
    "        env = env_fn()\n",
    "        env._SET_RENDERING_VIEWPORT_SIZE(4000, 2000, keep_ratio=True)\n",
    "        \n",
    "        if test_set is None:\n",
    "            test_set_params, rewards = test_policy.load_training_test_set(data_path, order_by_best_rewards=args.bests)\n",
    "        else:\n",
    "            test_set_params = test_policy.load_fixed_test_set(data_path, test_set)\n",
    "            rewards = ep_returns[i]\n",
    "        result[setting[\"expe_name\"]] = [param_dict_to_param_vec(param_bounds, param) for param in test_set_params]\n",
    "        \n",
    "        if nb_tasks == -1:\n",
    "            nb_tasks = len(test_set_params)\n",
    "        \n",
    "        ordering_name = \"\"\n",
    "        if args.bests is None:\n",
    "            ordering_name = \"firsts\"\n",
    "        elif args.bests:\n",
    "            ordering_name = \"top\"\n",
    "        else:\n",
    "            ordering_name = \"worse\"\n",
    "        fig_name = \"{0}_s{1}_{2}test-set-analysis_{3}_{4}\".format(args.expe_name, \n",
    "                                                               current_expe_best_seed, \n",
    "                                                               \"fixed-\" if test_set is not None else \"\",\n",
    "                                                               ordering_name,\n",
    "                                                               nb_tasks)\n",
    "        plot_test_tasks_results(env, test_set_params, rewards, fig_name, nb_env_test_to_check=nb_tasks)\n",
    "        env.close()\n",
    "        i+=1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Curriculum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stump Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    new_cmap = mcolors.LinearSegmentedColormap.from_list(\n",
    "        'trunc({n},{a:.2f},{b:.2f})'.format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap\n",
    "\n",
    "def draw_ellipse(position, covariance, ax=None, edge_color=None, face_color=None, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    covariance = covariance[0:2,0:2]\n",
    "    position = position[0:2]\n",
    "\n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "\n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(2, 3):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "                             angle, **kwargs, edgecolor=edge_color, facecolor=face_color))\n",
    "\n",
    "def get_colorbar(cmap, ax):\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = ax.figure\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    cbar = fig.colorbar(cmap, cax=cax)\n",
    "    plt.sca(ax)\n",
    "    return cbar; cax\n",
    "\n",
    "def plot_gmm(weights, means, covariances, X=None, ax=None, xlim=[0,1], ylim=[0,1], xlabel='', ylabel='',\n",
    "             bar=True, bar_side='right'):\n",
    "    \"\"\"Draw distributions of a GMM\"\"\"\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    ft_off = 15\n",
    "\n",
    "    ax = ax or plt.gca()\n",
    "    colormap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "    cmap = truncate_colormap(colormap, minval=0.0,maxval=1.0)\n",
    "    for pos, covar, w in zip(means, covariances, weights):\n",
    "        draw_ellipse(pos, covar, alpha=0.6, ax=ax, edge_color=cmap(pos[-1]), face_color=cmap(pos[-1]))\n",
    "\n",
    "    if bar:\n",
    "#         divider = make_axes_locatable(ax)\n",
    "#         cax = divider.append_axes(\"right\", size=\"5%\", pad=0.5)\n",
    "        cax, _ = cbar.make_axes(ax, location=bar_side, shrink=0.8)\n",
    "        cb = cbar.ColorbarBase(cax, cmap=cmap)\n",
    "        cb.set_label('Absolute Learning Progress', fontsize=ft_off + 5)\n",
    "        cax.tick_params(labelsize=ft_off + 0)\n",
    "        cax.yaxis.set_ticks_position(bar_side)\n",
    "        cax.yaxis.set_label_position(bar_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stumps_curriculum(dataset_folder, settings, frequency=1, initial_frequency=250000,\n",
    "                           stump_height_dims=[-1, 4], stump_spacing_dims=[-1, 7]):\n",
    "    \"\"\"\n",
    "        Generate a gif visualization of the curriculum of best seeds of chosen experiments.\n",
    "        \n",
    "        This function uses book-keeped 100 tasks sampled by teachers every `initial_frequency` steps. \n",
    " \n",
    "        :param dataset_folder: Directory containing experiments to load (do not forget '/' at the end of the path)\n",
    "        :param settings: Dictionary defining experiments to load \n",
    "        :param frequency: Plot 1/N sampled distribution \n",
    "        :param initial_frequency: Initial frequency (steps) at which the tasks were sampled\n",
    "        :param stump_height_dims: Bounds of the stump height axis\n",
    "        :param stump_spacing_dims: Bounds of the stump spacing axis\n",
    "    \"\"\"\n",
    "    parser = test_policy.get_parser()\n",
    "    parser.add_argument('--expe_name', type=str)\n",
    "    \n",
    "    result = {}\n",
    "    for setting in settings:\n",
    "        current_expe_best_seed = best_seeds[setting[\"expe_name\"]]\n",
    "        \n",
    "        args_str = dict_to_args_str(setting)\n",
    "        args = parser.parse_args(args_str)\n",
    "        _, param_bounds, _, _ = EnvironmentArgsHandler.get_object_from_arguments(args)\n",
    "        \n",
    "        current_label = labels[setting[\"expe_name\"]]\n",
    "        data = models_saves[setting[\"expe_name\"]]['data'][current_expe_best_seed]\n",
    "        task_samples = data[\"periodical_samples\"]\n",
    "        associated_infos = data[\"periodical_infos\"]\n",
    "        filenames = []\n",
    "        \n",
    "        for i in range(0, len(task_samples), frequency):\n",
    "            if len(task_samples[i]) == 0:\n",
    "                continue\n",
    "                \n",
    "            f, ax = plt.subplots(1,1,figsize=(20,20))\n",
    "            current_data = task_samples[i]\n",
    "            infos = associated_infos[i]\n",
    "            hue = None\n",
    "            ax.set_ylabel('stump_spacing', fontsize=25)\n",
    "            ax.set_xlabel('stump_height', fontsize=25)\n",
    "            plt.xticks(fontsize=18)\n",
    "            plt.yticks(fontsize=18)\n",
    "            plt.xlim(stump_height_dims[0], stump_height_dims[1])\n",
    "            plt.ylim(stump_spacing_dims[0], stump_spacing_dims[1])\n",
    "            set_legend = lambda: None\n",
    "            bk_index = infos[0][\"bk_index\"]\n",
    "            \n",
    "            # CHANGE THIS IF YOU ADD A NEW TEACHER #\n",
    "            if current_label in [\"ALP-GMM\", \"Covar-GMM\"]:\n",
    "                if bk_index > 0:\n",
    "                    plot_gmm(data[\"weights\"][bk_index], data[\"means\"][bk_index], data[\"covariances\"][bk_index], \n",
    "                             ax=ax, xlim=stump_height_dims, ylim=stump_spacing_dims)\n",
    "            elif current_label == \"Self-Paced\":\n",
    "                draw_ellipse(data[\"mean\"][bk_index], data[\"covariance\"][bk_index], ax=ax, alpha=0.5)\n",
    "            elif current_label == \"ADR\":\n",
    "                x1 = data[\"task_space\"][bk_index][0][0]\n",
    "                x2 = data[\"task_space\"][bk_index][1][0]\n",
    "                y1 = data[\"task_space\"][bk_index][0][1]\n",
    "                y2 = data[\"task_space\"][bk_index][1][1]\n",
    "                ax.add_patch(Rectangle((x1, y1), x2-x1, y2-y1, alpha=0.5))\n",
    "            elif current_label == \"Setter-Solver\":\n",
    "                set_legend = lambda: ax.legend(title=\"Feasibility\", fontsize=25)\n",
    "                hue = [_info[\"task_infos\"][0][0] for _info in infos]\n",
    "            elif current_label == \"RIAC\":\n",
    "#                 for box in data[\"all_boxes\"][bk_index]:\n",
    "#                     ax.add_patch(Rectangle((box.low[0], box.high[1]), \n",
    "#                                            box.low[1]-box.low[0], \n",
    "#                                            box.high[1]-box.high[0], \n",
    "#                                            alpha=0.5, color=))     \n",
    "                set_legend = lambda: ax.legend(title=\"Region ALP\", fontsize=25)\n",
    "                hue = [data[\"all_alps\"][bk_index][_info[\"task_infos\"]] for _info in infos]\n",
    "        # CHANGE THIS IF YOU ADD A NEW TEACHER #\n",
    "            \n",
    "            g = sns.scatterplot(x=current_data[:, 0], y=current_data[:, 1], ax=ax, hue=hue, s=100)\n",
    "            legend = set_legend()\n",
    "            if legend is not None:\n",
    "                legend.get_title().set_fontsize('25')\n",
    "                for legobj in legend.legendHandles:\n",
    "                    legobj.set_linewidth(5.0)\n",
    "            f_name = \"../TeachMyAgent/graphics/gifs/scatter_{}.png\".format(i)\n",
    "            plt.suptitle('Step {}'.format(math.ceil(initial_frequency/frequency) * i), fontsize=25)\n",
    "            plt.savefig(f_name, bbox_inches='tight')\n",
    "            plt.close(f)\n",
    "            filenames.append(f_name)\n",
    "        \n",
    "        images = []\n",
    "        for filename in filenames:\n",
    "            images.append(imageio.imread(filename))\n",
    "        imageio.mimsave('TeachMyAgent/graphics/{}.gif'.format(setting[\"expe_name\"] + \"_\" + str(current_expe_best_seed)), images, duration=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_parkour_curriculum(dataset_folder, settings, frequency=1, initial_frequency=250000):\n",
    "    \"\"\"\n",
    "        Generate a gif visualization of the curriculum of best seeds of chosen experiments.\n",
    "        \n",
    "        This function uses book-keeped 100 tasks sampled by teachers every `initial_frequency` steps. \n",
    " \n",
    "        :param dataset_folder: Directory containing experiments to load (do not forget '/' at the end of the path)\n",
    "        :param settings: Dictionary defining experiments to load \n",
    "        :param frequency: Plot 1/N sampled distribution \n",
    "        :param initial_frequency: Initial frequency (steps) at which the tasks were sampled\n",
    "    \"\"\"\n",
    "    parser = test_policy.get_parser()\n",
    "    parser.add_argument('--expe_name', type=str)\n",
    "    \n",
    "    result = {}\n",
    "    for setting in settings:\n",
    "        current_expe_best_seed = best_seeds[setting[\"expe_name\"]]\n",
    "        data_path = os.path.join(dataset_folder, setting[\"expe_name\"], setting[\"expe_name\"] + \"_s\" + str(current_expe_best_seed))\n",
    "        setting[\"fpath\"] = data_path\n",
    "    \n",
    "        args_str = dict_to_args_str(setting)\n",
    "\n",
    "        args = parser.parse_args(args_str)\n",
    "        env_fn, param_bounds, _, _ = EnvironmentArgsHandler.get_object_from_arguments(args)\n",
    "        env = env_fn()\n",
    "        env._SET_RENDERING_VIEWPORT_SIZE(4000, 2000, keep_ratio=True)\n",
    "        \n",
    "        fig_name = \"{0}_s{1}_curriculum-analysis\".format(args.expe_name, \n",
    "                                                               current_expe_best_seed)\n",
    "        data = models_saves[setting[\"expe_name\"]]['data'][current_expe_best_seed]\n",
    "        tasks = data[\"periodical_samples\"]\n",
    "        associated_infos = data[\"periodical_infos\"]\n",
    "        \n",
    "        nb_env = math.ceil(len(tasks) / frequency)\n",
    "        \n",
    "        filenames = []\n",
    "        \n",
    "        for i in range(0, nb_env-1, frequency):\n",
    "            if len(tasks[i]) == 0:\n",
    "                continue\n",
    "            current_tasks = tasks[i]\n",
    "            current_infos = associated_infos[i]\n",
    "            index = random.randint(0, len(current_tasks)-1)\n",
    "            task = param_vec_to_param_dict(param_bounds, current_tasks[index])\n",
    "            associated_info = current_infos[index]\n",
    "            f, ax = plt.subplots(1,1,figsize=(12,10))\n",
    "\n",
    "            env.set_environment(**task)\n",
    "            env.reset()\n",
    "\n",
    "            plt.imshow(env.render(mode='rgb_array'))\n",
    "            plt.axis('off')\n",
    "            f_name = \"../TeachMyAgent/graphics/gifs/{}_{}.png\".format(fig_name, i)\n",
    "            plt.suptitle('Step {}'.format(math.ceil(initial_frequency/frequency) * i), fontsize=20)\n",
    "            plt.savefig(f_name, bbox_inches='tight')\n",
    "            plt.close(f)\n",
    "            filenames.append(f_name)\n",
    "\n",
    "        images = []\n",
    "        for filename in filenames:\n",
    "            images.append(imageio.imread(filename))\n",
    "        imageio.mimsave('../TeachMyAgent/graphics/{}.gif'.format(setting[\"expe_name\"] + \"_\" + str(current_expe_best_seed)), images, duration=0.3)\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curriculum analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change settings below for each experiment: \n",
    "- for Stump Tracks, you only need to specify the experiment's name\n",
    "- for thre Parkour, you must specify the experiment's name as well as the agent's type and its lidars "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parkour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_parkour_curriculum(data_folder, settings = [\n",
    "    {\n",
    "        \"env\": \"parametric-continuous-parkour-v0\",\n",
    "        \"embodiment\": \"old_classic_bipedal\",\n",
    "        \"lidars_type\": \"down\",\n",
    "        \"expe_name\" : \"14-12_benchmark_parkour_Setter-Solver_walker_type_old_classic_bipedal\",\n",
    "    }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_parkour_curriculum(data_folder, settings = [\n",
    "    {\n",
    "        \"env\": \"parametric-continuous-parkour-v0\",\n",
    "        \"embodiment\": \"climbing_profile_chimpanzee\",\n",
    "        \"lidars_type\": \"up\",\n",
    "        \"expe_name\" : \"14-12_benchmark_parkour_Setter-Solver_walker_type_climbing_profile_chimpanzee\",\n",
    "    }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_parkour_curriculum(data_folder, settings = [\n",
    "    {\n",
    "        \"env\": \"parametric-continuous-parkour-v0\",\n",
    "        \"embodiment\": \"fish\",\n",
    "        \"lidars_type\": \"full\",\n",
    "        \"expe_name\" : \"14-12_benchmark_parkour_Setter-Solver_walker_type__fish\",\n",
    "    }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stump Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stumps_curriculum(data_folder, settings = [\n",
    "    {\n",
    "        \"expe_name\" : \"14-12_profiling_benchmark_stumps_Setter-Solver_criteria_1_allow_expert_knowledge_maximal\",\n",
    "    }], stump_height_dims=[-1, 10], stump_spacing_dims=[-1, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stumps_curriculum(data_folder, settings = [\n",
    "    {\n",
    "        \"expe_name\" : \"14-12_profiling_benchmark_stumps_Setter-Solver_criteria_2_allow_expert_knowledge_maximal\",\n",
    "    }], stump_height_dims=[-4, 4], stump_spacing_dims=[-1, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_stumps_curriculum(data_folder, settings = [\n",
    "    {\n",
    "        \"expe_name\" : \"14-12_profiling_benchmark_stumps_Setter-Solver_criteria_3_allow_expert_knowledge_maximal\",\n",
    "    }], stump_height_dims=[-1, 4], stump_spacing_dims=[-1, 7])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
