{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "This notebook is made to help analysing results produced by TeachMyAgent's experiments. Using this, one can reproduce the figures we provide in our [paper](https://developmentalsystems.org/TeachMyAgent/). \n",
    "\n",
    "## How to use this notebook\n",
    "This notebook is broken down into 4 sections:\n",
    "- **Imports**: import needed packages.\n",
    "- **Load Data**: load results produced by experiments and format them (e.g. calculate percentage of mastered tasks).\n",
    "- **Plot definitions**: define all the plot functions we provide.\n",
    "- **Experiment graphs**: use the previously defined functions to generate the different figures we show in our paper.\n",
    "\n",
    "## Add our paper's results to your plots\n",
    "In order to add the results we provide in our paper to your plots, make sure you have downloaded them:\n",
    "1. Go to the `notebooks` folder\n",
    "2. Make the `download_baselines.sh` script executable: `chmod +x download_baselines.sh`\n",
    "3. Download results: `./download_baselines.sh`\n",
    "> **_WARNING:_**  This will download a zip weighting approximayely 4.5GB. Then, our script will extract the zip file in `TeachMyAgent/data`. Once extracted, results will weight approximately 15GB. \n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "import math\n",
    "import pylab\n",
    "import copy\n",
    "import re\n",
    "from enum import Enum\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "DIV_LINE_WIDTH = 50\n",
    "print(np.__version__)\n",
    "print(sys.executable)\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from TeachMyAgent.students.run_logs_util import get_run_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_datasets(rootdir, name_filter=None, rename_labels=False):\n",
    "    \"\"\"\n",
    "        Loads results of experiments.\n",
    " \n",
    "        Results to load can be filtered by their name and each experiment can be associated to a label (usually ACL method's name)\n",
    " \n",
    "        :param rootdir: Directory containing experiments to load (do not forget '/' at the end of the path)\n",
    "        :param name_filter: String experiments to load must contain\n",
    "        :param rename_labels: If True, each experiment will be associated to a label (see below). Labels are the names that will appear in plots.\n",
    "        :type rootdir: str\n",
    "        :type name_filter: str (or None)\n",
    "        :type rename_labels: boolean\n",
    "    \"\"\"\n",
    "    global default_colors_palette\n",
    "    _, models_list, _ = next(os.walk(rootdir))\n",
    "    print(models_list)\n",
    "    for dir_name in models_list.copy():\n",
    "        if \"ignore\" in dir_name:\n",
    "            models_list.remove(dir_name)\n",
    "        if name_filter is not None and name_filter not in dir_name:\n",
    "            models_list.remove(dir_name)\n",
    "            \n",
    "    # setting per-model type colors\n",
    "    if len(per_model_colors) == 0 and  len(models_list) > len(default_colors_palette):\n",
    "        default_colors_palette = sns.color_palette(\"hls\", len(models_list))\n",
    "        \n",
    "    for i,m_name in enumerate(models_list):\n",
    "        for m_type, m_color in per_model_colors.items():\n",
    "            if m_type in m_name:\n",
    "                colors[m_name] = m_color\n",
    "        if m_name not in colors:\n",
    "            colors[m_name] = default_colors_palette[i]\n",
    "            \n",
    "        print(\"extracting data for {}...\".format(m_name))\n",
    "        m_id = m_name\n",
    "        models_saves[m_id] = OrderedDict()\n",
    "        models_saves[m_id]['data'] = get_run_logs(rootdir+m_name, book_keeping_keys=['env_test_rewards'], min_len=0)\n",
    "        print(\"done\")\n",
    "        if m_name not in labels:\n",
    "            if not rename_labels:\n",
    "                labels[m_name] = m_name\n",
    "            else:\n",
    "                ##### MODIFY THIS IF YOU ADD A NEW METHOD #####\n",
    "                if 'ADR' in m_name:\n",
    "                    labels[m_name] = 'ADR'\n",
    "                elif 'ALP-GMM' in m_name:\n",
    "                    labels[m_name] = 'ALP-GMM'\n",
    "                elif 'Random' in m_name:\n",
    "                    labels[m_name] = 'Random'\n",
    "                elif 'Covar-GMM' in m_name:\n",
    "                    labels[m_name] = 'Covar-GMM'\n",
    "                elif 'RIAC' in m_name:\n",
    "                    labels[m_name] = 'RIAC'\n",
    "                elif 'GoalGAN' in m_name:\n",
    "                    labels[m_name] = 'GoalGAN'\n",
    "                elif 'Self-Paced' in m_name:\n",
    "                    labels[m_name] = 'Self-Paced'\n",
    "                elif 'Setter-Solver' in m_name:\n",
    "                    labels[m_name] = 'Setter-Solver'\n",
    "                elif 'UPPER_BASELINE' in m_name:\n",
    "                    labels[m_name] = 'UPPER_BASELINE'\n",
    "                else:\n",
    "                    labels[m_name] = m_name\n",
    "                ##### MODIFY THIS IF YOU ADD A NEW METHOD #####\n",
    "                \n",
    "labels = OrderedDict()\n",
    "default_colors_palette = sns.color_palette('colorblind')\n",
    "\n",
    "##### MODIFY THIS IF YOU ADD A NEW METHOD #####\n",
    "# per_model_colors = OrderedDict() # Set it to empty if you want one color per experiment instead of one color per ACL method\n",
    "per_model_colors = OrderedDict([('ALP-GMM', default_colors_palette[0]),\n",
    "                                ('Covar-GMM', default_colors_palette[1]),\n",
    "                                ('ADR', default_colors_palette[2]),\n",
    "                                ('Random', default_colors_palette[3]),\n",
    "                                ('RIAC', default_colors_palette[4]),\n",
    "                                ('GoalGAN', default_colors_palette[9]),\n",
    "                                ('Self-Paced', default_colors_palette[6]),\n",
    "                                ('Setter-Solver', default_colors_palette[7]),\n",
    "                                ('UPPER_BASELINE', default_colors_palette[5])])\n",
    "##### MODIFY THIS IF YOU ADD A NEW METHOD #####\n",
    "\n",
    "models_saves = OrderedDict()\n",
    "colors = OrderedDict()\n",
    "\n",
    "##### MODIFY THIS TO POINT TO YOUR DATA FOLDER #####\n",
    "data_folder = \"../TeachMyAgent/data/BENCHMARK/\"\n",
    "##### MODIFY THIS TO POINT TO YOUR DATA FOLDER #####\n",
    "\n",
    "get_datasets(data_folder, rename_labels=True)\n",
    "# get_datasets(data_folder, rename_labels=False, name_filter=\"ADR\") # You can also add filters\n",
    "\n",
    "# order runs for legend order as in per_models_colors, with corresponding colors\n",
    "if len(per_model_colors) > 0:\n",
    "    ordered_labels = OrderedDict()\n",
    "    for teacher_type in per_model_colors.keys():\n",
    "        for k,v in labels.items():\n",
    "            if teacher_type in k:\n",
    "                ordered_labels[k] = v\n",
    "    labels = ordered_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle baseline Random teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format experiments of Random teacher to make it appear in all expert knowledge setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "default_configuration = \"no\"\n",
    "configurations_to_add = [\"minimal\", \"maximal\"]\n",
    "new_expes_to_add = {}\n",
    "for expe_id in models_saves:\n",
    "    if \"profiling_benchmark_stumps_Random\" in expe_id:\n",
    "        for new_config in configurations_to_add:\n",
    "            new_expe_id = expe_id.replace(\"allow_expert_knowledge_\" + default_configuration,\n",
    "                                          \"allow_expert_knowledge_\" + new_config)\n",
    "            new_expes_to_add[new_expe_id] = OrderedDict()\n",
    "            new_expes_to_add[new_expe_id]['data'] = copy.copy(models_saves[expe_id]['data'])\n",
    "            labels[new_expe_id] = labels[expe_id]\n",
    "            colors[new_expe_id] = colors[expe_id]\n",
    "models_saves.update(new_expes_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Upper Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format names of upper baseline experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_to_add = [\"1\", \"2\", \"3\", \"4\"]\n",
    "new_expes_to_add = {}\n",
    "for expe_id in models_saves:\n",
    "    if \"UPPER_BASELINE\" in expe_id:\n",
    "        for criterion in criteria_to_add:\n",
    "            new_expe_id = expe_id.replace(\"UPPER_BASELINE\",\n",
    "                                          \"UPPER_BASELINE_criteria_\" + criterion)\n",
    "            new_expes_to_add[new_expe_id] = OrderedDict()\n",
    "            new_expes_to_add[new_expe_id]['data'] = copy.copy(models_saves[expe_id]['data'])\n",
    "            labels[new_expe_id] = labels[expe_id]\n",
    "            colors[new_expe_id] = colors[expe_id]\n",
    "models_saves.update(new_expes_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge experiments by teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some experiments (e.g. on parkour or criteria 5) were broken down into multiple experiments. In order to analyze them, the results must be merged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAREFUL: IT NEEDS RENAME_LABELS SET TO TRUE\n",
    "def merge_experiments_by_teacher(experiment_name, result_name=None):\n",
    "    new_saves = OrderedDict()\n",
    "    anything_token = \"*\"\n",
    "    anything_pattern = \"[a-zA-z0-9\\-.]*\"\n",
    "    experiment_name_regex_pattern = experiment_name.replace(anything_token, anything_pattern)\n",
    "    regex = re.compile(experiment_name_regex_pattern)\n",
    "    for expe_id in models_saves:\n",
    "        if regex.match(expe_id):\n",
    "            associated_label = labels[expe_id]\n",
    "            if result_name is None:\n",
    "                new_expe_name = experiment_name.replace('*', '').replace('|', ',') + \"_\" + associated_label\n",
    "            else:\n",
    "                new_expe_name = result_name.replace('{LABEL}', associated_label)\n",
    "\n",
    "            if models_saves[expe_id]['data'] is not None:\n",
    "                if new_expe_name not in new_saves:\n",
    "                    new_saves[new_expe_name] = OrderedDict()\n",
    "                    new_saves[new_expe_name]['data'] = copy.copy(models_saves[expe_id]['data'])\n",
    "                    labels[new_expe_name] = associated_label\n",
    "                    colors[new_expe_name] = colors[expe_id]\n",
    "                else:\n",
    "                    new_saves[new_expe_name]['data'].extend(copy.copy(models_saves[expe_id]['data']))\n",
    "    models_saves.update(new_saves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_experiments_by_teacher(\"*benchmark_parkour*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_experiments_by_teacher(\"*profiling_benchmark_stumps_*_criteria_5*allow_expert_knowledge_no*\", \n",
    "                             result_name=\"profiling_benchmark_stumps_{LABEL}_criteria_5_allow_expert_knowledge_no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_experiments_by_teacher(\"*profiling_benchmark_stumps_*_criteria_5*allow_expert_knowledge_minimal*\", \n",
    "                             result_name=\"profiling_benchmark_stumps_{LABEL}_criteria_5_allow_expert_knowledge_minimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_experiments_by_teacher(\"*profiling_benchmark_stumps_*_criteria_5*allow_expert_knowledge_maximal*\", \n",
    "                             result_name=\"profiling_benchmark_stumps_{LABEL}_criteria_5_allow_expert_knowledge_maximal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute mastered tasks percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute \"% of Mastered tasks\" metric: percentage of test tasks (over a test set of 100 tasks) on which the agent obtained an episodic reward greater than a threshold (230)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mastered_thr = 230\n",
    "for i,(m_id,label) in enumerate(labels.items()):\n",
    "    print(m_id)\n",
    "    runs_data = models_saves[m_id]['data']\n",
    "    #collect raw perfs\n",
    "    print(\"Seeds : \" + str(len(runs_data)))\n",
    "    for r,run in enumerate(runs_data):\n",
    "        models_saves[m_id]['data'][r]['nb_mastered'] = []\n",
    "        models_saves[m_id]['data'][r]['avg_pos_rewards'] = []\n",
    "        models_saves[m_id]['data'][r]['local_rewards'] = []\n",
    "        if 'env_test_rewards' in run:\n",
    "            size_test_set = int(len(run['env_test_rewards'])/len(run['evaluation return']))\n",
    "            for j in range(len(run['evaluation return'])):#max_epoch):\n",
    "                test_data = np.array(run['env_test_rewards'][j*size_test_set:(j+1)*(size_test_set)])\n",
    "                nb_mastered = len(np.where(test_data > mastered_thr)[0])\n",
    "                models_saves[m_id]['data'][r]['nb_mastered'].append((nb_mastered/size_test_set)*100)\n",
    "        else:\n",
    "            print(\"Skipping seed {}\".format(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_welch(d1, d2):\n",
    "    \"\"\"\n",
    "        Calculate Welch's t-test between two distributions.\n",
    " \n",
    "        :param d1: Distribution 1\n",
    "        :param d2: Distribution 2\n",
    "        :type d1: array (e.g. list, ndarray)\n",
    "        :type d2: array (e.g. list, ndarray)\n",
    "        :return: T-statistic and p-value\n",
    "        :rtype: tuple\n",
    "    \"\"\"\n",
    "    return ss.ttest_ind(d1, d2, equal_var=False)\n",
    "\n",
    "def get_multistep_welch(algo_0, algo_1):\n",
    "    \"\"\"\n",
    "        Calculate Welch's t-test between two list of distributions index-wise.\n",
    " \n",
    "        :param algo_0: List of distributions 1\n",
    "        :param algo_1: List of distributions 2\n",
    "        :type algo_0: list of array (e.g. list, ndarray)\n",
    "        :type algo_1: list of array (e.g. list, ndarray)\n",
    "        :return: List of t-test results (of each index) and max value of the two distributions at each index\n",
    "        :rtype: tuple\n",
    "    \"\"\"\n",
    "    tt_test = []\n",
    "    max_values =  []\n",
    "    for i in range(min(algo_0.shape[1], algo_1.shape[1])):\n",
    "        d1 = [v[i] for v in algo_0]\n",
    "        d2 = [v[i] for v in algo_1]\n",
    "        max_values.append(max(d1 + d2))\n",
    "        tt_test.append(get_simple_welch(d1, d2))\n",
    "        \n",
    "    return tt_test, max_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single curve plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_shade(subplot_nb, ax,x,y,err,color,shade_color,label,\n",
    "                  legend=False, leg_size=30, leg_loc='best', title=None,\n",
    "                  ylim=[0,100], xlim=[0,40], leg_args={}, leg_linewidth=8.0, linewidth=7.0,\n",
    "                  ticksize=30, y_label='% Mastered env', label_size=30):\n",
    "    \"\"\"\n",
    "        Plot a curve along with its error (shade).\n",
    " \n",
    "        :param subplot_nb: Index in multi-plots figure\n",
    "        :param ax: Matplotlib axis\n",
    "        :param x: List of x positions\n",
    "        :param y: List associated y\n",
    "        :param color: Curve color \n",
    "        :param shade_color: Shade color\n",
    "        :param label: Label name of the curve\n",
    "        :param legend: Whether legend should be printed\n",
    "        :param leg_size: Legend size\n",
    "        :param leg_loc: Legend position\n",
    "        :param title: Plot title\n",
    "        :param ylim: Limits of the y axis\n",
    "        :param xlim: Limits of the x axis\n",
    "        :param leg_args: Matplotlib legend arguments\n",
    "        :param leg_linewidth: Legend's line width\n",
    "        :param linewidth: Curve's width\n",
    "        :param ticksize: Axis tick size\n",
    "        :param y_label: Y axis label\n",
    "        :param label_size: Label size in legend\n",
    "    \"\"\"\n",
    "    ax.locator_params(axis='x', nbins=5)\n",
    "    ax.locator_params(axis='y', nbins=5)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=ticksize)\n",
    "    ax.plot(x,y, color=color, label=label,linewidth=linewidth)\n",
    "    ax.fill_between(x,y-err,y+err,color=shade_color,alpha=0.2)\n",
    "    if legend:\n",
    "        leg = ax.legend(loc=leg_loc, fontsize=leg_size, **leg_args) #34\n",
    "        for legobj in leg.legendHandles:\n",
    "            legobj.set_linewidth(leg_linewidth)\n",
    "    ax.set_xlabel('Agent steps (millions)', fontsize=label_size)\n",
    "    if subplot_nb == 0:\n",
    "        ax.set_ylabel(y_label, fontsize=label_size)\n",
    "    ax.set_xlim(xmin=xlim[0],xmax=xlim[1])\n",
    "    ax.set_ylim(bottom=ylim[0],top=ylim[1])\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=22)\n",
    "\n",
    "def plot_all_and_median(subplot_nb, ax,x,ys,color,label,\n",
    "                         y_min=None,y_max=None, legend=False, title=None, x_max=20, y_label='% Mastered env'):\n",
    "    \"\"\"\n",
    "        Plot curves along with their median.\n",
    " \n",
    "        :param subplot_nb: Index in multi-plots figure\n",
    "        :param ax: Matplotlib axis\n",
    "        :param x: List of x positions\n",
    "        :param y: List associated ys\n",
    "        :param color: Curve color \n",
    "        :param label: Label name of the curve\n",
    "        :param y_min: Minimum of the y axis\n",
    "        :param y_max: Maximum of the y axis\n",
    "        :param legend: Whether legend should be printed\n",
    "        :param title: Plot title\n",
    "        :param x_max: Maximum of the x axis\n",
    "        :param y_label: Y axis label\n",
    "    \"\"\"\n",
    "    \n",
    "    ax.locator_params(axis='x', nbins=5)\n",
    "    ax.locator_params(axis='y', nbins=5)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=30)\n",
    "    min_len = 999999\n",
    "    median = np.median(ys, axis=0)\n",
    "    for k,y in enumerate(ys):\n",
    "        ax.plot(x[0:min_len],y, color=color, linewidth=1.5, alpha=0.5)\n",
    "    ax.plot(x[0:min_len],median, color=color, linewidth=7 , label=label)\n",
    "    if legend:\n",
    "        leg = ax.legend(loc='best', fontsize=25)\n",
    "    ax.set_xlabel('Million steps', fontsize=18)\n",
    "    if subplot_nb == 0:\n",
    "        ax.set_ylabel(y_label, fontsize=18)\n",
    "    ax.set_xlim(xmin=0,xmax=x_max)\n",
    "    if y_min is not None:\n",
    "        ax.set_ylim(bottom=y_min,top=y_max)\n",
    "    else:\n",
    "        ax.set_ylim(top=100)\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple curves plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Statistical_tests_enum(Enum):\n",
    "    \"\"\"\n",
    "        Statistical test type.\n",
    " \n",
    "        MULTI_STEP: T-test must be computed between the two distributions at each time step\n",
    "        MULTI_STEP: T-test must be computed between the two distributions only at the last time step\n",
    "    \"\"\"\n",
    "    NONE = 0\n",
    "    MULTI_STEP = 1\n",
    "    LAST_STEP = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(agent_type, plot_type='shade', metric='nb_mastered', legend=True, y_min=0, y_max=100, x_max=10, leg_size=20, leg_loc='best', new_labels=None,\n",
    "                allow_different_sizes=False, _ax=None, welch=Statistical_tests_enum.NONE, welch_baseline='all', welch_p_threshold=0.05, welch_step_freq=1):\n",
    "    \"\"\"\n",
    "         Plot curves of experiments whose name matches `agent_type` regex.\n",
    " \n",
    "        :param agent_type: Regex filtering experiments to show\n",
    "        :param plot_type: 'shade' (mean + std) OR 'shade_se' (mean + standard error of the mean) OR 'all_and_median' (all curves + median)\n",
    "        :param metric: Metric to plot\n",
    "        :param legend: Whether legend should be printed\n",
    "        :param y_min: Minimum of the y axis\n",
    "        :param y_max: Maximum of the y axis\n",
    "        :param x_max: Maximum of the x axis\n",
    "        :param leg_size: Legend size\n",
    "        :param leg_loc: Legend position\n",
    "        :param new_labels: Labels replacing the labels loaded when getting experients' logs in plots\n",
    "        :param allow_different_sizes: Allow curves to have different size (otherwise stop all curves at the length of the shortest)\n",
    "        :param _ax: Matplotlib axis\n",
    "        :param welch: Whether statistical tests should be computed (must be a `Statistical_tests_enum` value)\n",
    "        :param welch_baseline: Baseline every method should be compared to. If all combinations must be computed use 'all'\n",
    "        :param welch_p_threshold: P-value threshold to reject null hypothesis\n",
    "        :param welch_step_freq: Welch's test will be calculated every `welch_step_freq` steps if MULTI_STEP\n",
    "    \"\"\"\n",
    "    if _ax is None:\n",
    "        f, ax = plt.subplots(1,1,figsize=(30,12))\n",
    "    else:\n",
    "        ax = _ax\n",
    "    ys_for_weclh = {}\n",
    "    episodes_per_experiment = {}\n",
    "    max_y = -1\n",
    "    anything_token = \"*\"\n",
    "    anything_pattern = \"[a-zA-z0-9\\-.]*\"\n",
    "    agent_type_regex_pattern = agent_type.replace(anything_token, anything_pattern)\n",
    "    regex = re.compile(agent_type_regex_pattern)\n",
    "    _labels = new_labels if new_labels is not None else labels\n",
    "    for i,(m_id,label) in enumerate(_labels.items()):\n",
    "        if regex.match(m_id):\n",
    "            runs_data = models_saves[m_id]['data']\n",
    "            ys = []\n",
    "            episodes = []\n",
    "            nb_seeds = len(runs_data)\n",
    "            if nb_seeds > 0:\n",
    "                for run in runs_data:  \n",
    "                    data = run[metric]\n",
    "                    if len(run['total timesteps']) > len(episodes):\n",
    "                        episodes = np.array(run['total timesteps'])\n",
    "                    ys.append(data)\n",
    "                if not allow_different_sizes:\n",
    "                    #clean data in case an expe has seeds with varying epoch number    \n",
    "                    min_len = 999999\n",
    "                    for y in ys:\n",
    "                        if len(y) < min_len:\n",
    "                            min_len = len(y)\n",
    "                    ys_same_len = np.empty((len(ys), min_len))\n",
    "                    for i in range(len(ys)):\n",
    "                        y = ys[i]\n",
    "                        for j in range(min_len):\n",
    "                            ys_same_len[i, j] = y[j]\n",
    "                    episodes = episodes[0:min_len]\n",
    "                else:\n",
    "                    full_len = max([len(y) for y in ys])\n",
    "                    ys_same_len = np.ma.empty((len(ys), full_len))\n",
    "                    ys_same_len.mask = True\n",
    "                    for i in range(len(ys)):\n",
    "                        y = ys[i]\n",
    "                        for j in range(len(y)):\n",
    "                            ys_same_len[i, j] = y[j]\n",
    "                episodes = [e/1000000 for e in episodes]\n",
    "                episodes_per_experiment[m_id] = episodes\n",
    "                \n",
    "                if ys_same_len.size > 0:\n",
    "                    if welch != Statistical_tests_enum.NONE:\n",
    "                        ys_for_weclh[m_id] = ys_same_len\n",
    "\n",
    "                    if plot_type in [\"shade\", \"shade_se\"]:\n",
    "                        stds = ys_same_len.std(axis=0)\n",
    "                        if plot_type == \"shade_se\":\n",
    "                            stds = stds / math.sqrt(nb_seeds)\n",
    "                        means = ys_same_len.mean(axis=0)\n",
    "                        max_y = max(max_y, max(means + stds))\n",
    "                        plot_with_shade(0, ax, episodes, means, stds, colors[m_id],\n",
    "                                        colors[m_id], label, leg_loc=leg_loc, y_label=metric, leg_args={\"frameon\":False},\n",
    "                                        legend=legend, ylim=[y_min,y_max], xlim=[0,x_max], leg_size=leg_size,\n",
    "                                        ticksize=40, label_size=40)\n",
    "                    elif plot_type == \"all_and_median\":\n",
    "                        plot_all_and_median(0, ax, episodes,ys_same_len,colors[m_id],label,\n",
    "                                            title=\"{}\".format(agent_type), legend=legend, x_max=x_max,\n",
    "                                            y_min=y_min, y_max=y_max, y_label=metric)\n",
    "    \n",
    "    if welch != Statistical_tests_enum.NONE:\n",
    "        # Get baseline(s) to compare with\n",
    "        if welch_baseline == 'all':\n",
    "            results_to_compare = ys_for_weclh.keys()\n",
    "        else:\n",
    "            if welch_baseline not in ys_for_weclh: raise Exception(\"Unrecognized baseline experiment name for welch's test\")\n",
    "            results_to_compare = [welch_baseline]\n",
    "        \n",
    "        i = 0  \n",
    "        for expe in ys_for_weclh:\n",
    "            j = 0\n",
    "            for expe_2 in results_to_compare:\n",
    "                if expe != expe_2:\n",
    "                    if welch == Statistical_tests_enum.MULTI_STEP:\n",
    "                        ttest_results, maxs = get_multistep_welch(ys_for_weclh[expe], ys_for_weclh[expe_2])\n",
    "                        k = 0\n",
    "                        max_val = max_y if max_y > 0 else max(maxs)\n",
    "                        for ttest in ttest_results:\n",
    "                            if (k+1) % welch_step_freq == 0 and ttest[1] < welch_p_threshold:\n",
    "                                ax.plot(episodes_per_experiment[expe][k], max_val + 2*(i+j) + 2, \n",
    "                                        '*', markersize=20, c=colors[expe])\n",
    "                            k += 1\n",
    "                    elif welch == Statistical_tests_enum.LAST_STEP:\n",
    "                        last_step = min(ys_for_weclh[expe].shape[1], ys_for_weclh[expe_2].shape[1]) - 1\n",
    "                        ttest_result = get_simple_welch(ys_for_weclh[expe][:,last_step], ys_for_weclh[expe_2][:,last_step])\n",
    "                        if ttest_result[1] < welch_p_threshold:\n",
    "                            ax.plot(episodes_per_experiment[expe][-1] + 0.5, ys_for_weclh[expe].mean(axis=0)[-1], \n",
    "                                    '*', markersize=20, c=colors[expe])\n",
    "                    else:\n",
    "                        raise Exception(\"Unrecognized statistical test.\")\n",
    "                    j += 1\n",
    "                                                    \n",
    "            if welch == Statistical_tests_enum.MULTI_STEP and welch_baseline == 'all' and len(ys_for_weclh) / 2 == i + 1:\n",
    "                break\n",
    "            i += 1\n",
    "                    \n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if _ax is None:\n",
    "        f.savefig('../TeachMyAgent/graphics/{0}_{1}_{2}.png'.\n",
    "                  format(agent_type.replace('*', '[]').replace('|', ','), plot_type, metric), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parkour's comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_comparisons(agent_type, metric='nb_mastered', y_min=0, y_max=100, x_max=20, allow_different_sizes=False, welch_p_threshold=0.05):\n",
    "    \"\"\"\n",
    "         Plot curves of experiments whose name matches `agent_type` regex as well as all curve-to-curve comparisons (i.e. figure 14 of our paper).\n",
    " \n",
    "        :param agent_type: Regex filtering experiments to show\n",
    "        :param metric: Metric to plot\n",
    "        :param y_min: Minimum of the y axis\n",
    "        :param y_max: Maximum of the y axis\n",
    "        :param x_max: Maximum of the x axis\n",
    "        :param allow_different_sizes: Allow curves to have different size (otherwise stop all curves at the length of the shortest)\n",
    "        :param welch_p_threshold: P-value threshold to reject null hypothesis\n",
    "    \"\"\"\n",
    "    anything_token = \"*\"\n",
    "    anything_pattern = \"[a-zA-z0-9\\-.]*\"\n",
    "    agent_type_regex_pattern = agent_type.replace(anything_token, anything_pattern)\n",
    "    regex = re.compile(agent_type_regex_pattern)\n",
    "    agents_to_plot = []\n",
    "    for i,(m_id,label) in enumerate(labels.items()):\n",
    "        if regex.match(m_id):\n",
    "            agents_to_plot.append(m_id)\n",
    "                     \n",
    "    nb_columns = len(agents_to_plot) + 1\n",
    "    nb_rows = len(agents_to_plot) + 2        \n",
    "    fig = plt.figure(constrained_layout=True, figsize=(35 + 5*nb_rows, 30 + 5*nb_columns))\n",
    "    widths = [0.2 if i == 0 else 1 for i in range(nb_columns)]\n",
    "    heights = [4, 0.2] + [1 for _ in range(nb_rows - 2)]\n",
    "    gs = fig.add_gridspec(nb_rows, nb_columns, width_ratios=widths, height_ratios=heights)\n",
    "    fig.patch.set_facecolor('#f7f7f7')\n",
    "    \n",
    "    f_0_0_ax = fig.add_subplot(gs[0, :])\n",
    "    plot_curves(agent_type, metric=metric, plot_type=\"shade_se\", welch=Statistical_tests_enum.NONE, _ax=f_0_0_ax, y_min=y_min, y_max=y_max, x_max=x_max, leg_size=50, allow_different_sizes=allow_different_sizes)\n",
    "    for i in range(len(agents_to_plot) + 1):\n",
    "        for j in range(len(agents_to_plot) + 1):\n",
    "            ax = fig.add_subplot(gs[i+1, j])\n",
    "            if i == 0 or j == 0:\n",
    "                ax.set_axis_off()\n",
    "                if i + j > 0:\n",
    "                    if i == 0:\n",
    "                        idx = j - 1\n",
    "                    else:\n",
    "                        idx = i - 1\n",
    "                    ax.text(0.5, 0.5, labels[agents_to_plot[idx]], ha=\"center\", va=\"center\", fontsize=50)\n",
    "            else:\n",
    "                if i > j:\n",
    "                    plot_curves(\"(\" + agents_to_plot[i - 1] + \"|\" + agents_to_plot[j - 1] + \")\", plot_type=\"shade_se\", leg_size=28,\n",
    "                                metric=metric, welch=Statistical_tests_enum.MULTI_STEP, welch_p_threshold=welch_p_threshold, _ax=ax, \n",
    "                                y_min=y_min, y_max=y_max, x_max=x_max, allow_different_sizes=allow_different_sizes)\n",
    "                else:\n",
    "                    ax.set_axis_off()\n",
    "                    \n",
    "    plt.savefig('../TeachMyAgent/graphics/comparisons_{}_{}.png'.format(\n",
    "        agent_type.replace('*', '[]').replace('|', ','), metric.replace(\" \", \"_\")), \n",
    "        facecolor='#f7f7f7', edgecolor='none', bbox_inches='tight', dpi=100)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stump Tracks radar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAREFUL: IT NEEDS RENAME_LABELS SET TO TRUE AS WELL AS PER MODEL COLORS ACTIVATED\n",
    "def generate_profile_chart(expe_template_name=\"(*profiling_benchmark_stumps_*|UPPER_BASELINE)_criteria_(1|2|3|4|5)_allow_expert_knowledge_(no|minimal|maximal)$\", \n",
    "                           baseline_teacher=\"Random\", list_of_teachers=None, tick_step=0.5, timestep=-1, p_value=0.05):\n",
    "    \"\"\"\n",
    "        Radar plots of Stump Tracks experiments (i.e. figure 3 and 9 of our paper).\n",
    " \n",
    "        :param expe_template_name: Regex filtering experiments to show (THIS CAN BE LEFT TO ITS DEFAULT VALUE)\n",
    "        :param baseline_teacher: Results are shown as an order of magnitude of the baseline teacher (default Random)\n",
    "        :param list_of_teachers: Filter teachers to show by their label (if None all teachers are plotted)\n",
    "        :param tick_step: Tick frequency\n",
    "        :param timestep: Plot results at a certain timestep of training (-1 means end of training)\n",
    "        :param p_value: P-value threshold to reject null hypothesis\n",
    "    \"\"\"\n",
    "    anything_token = \"*\"\n",
    "    anything_pattern = \"[a-zA-z0-9\\-.]*\"\n",
    "    experiment_name_regex_pattern = expe_template_name.replace(anything_token, anything_pattern)\n",
    "    name_regex = re.compile(experiment_name_regex_pattern)\n",
    "    criteria_regex = re.compile(anything_pattern + \"_criteria_[0-9]\")\n",
    "    \n",
    "    figname = \"benchmark_profiling{}\".format('_' + '_'.join(list_of_teachers) if list_of_teachers is not None else '')\n",
    "    if timestep != -1:\n",
    "        figname += \"_timestep-\" + str(timestep)\n",
    "    \n",
    "    criterion_label = [\n",
    "        \"Mostly unfeasible\\n task space\",\n",
    "        \"Mostly trivial\\n task space\",\n",
    "        \"Student that\\n forgets\",\n",
    "        \"Rugged\\n difficulty\",\n",
    "        \"Variety of\\n students\"\n",
    "    ]\n",
    "    \n",
    "    ek_types = [\"no\", \"low\", \"high\"]\n",
    "    \n",
    "    # CHANGE THIS IF YOU ADD A NEW TEACHER #\n",
    "    linestyle_tuple = [\n",
    "     ('solid',                 (0, ())),\n",
    "        \n",
    "     ('loosely dotted',        (0, (1, 1))),\n",
    "     ('dotted',                (0, (1, 0.5))),\n",
    "\n",
    "     ('loosely dashed',        (0, (5, 1.5))),\n",
    "     ('dashed',                (0, (5, 1))),\n",
    "     ('densely dashed',        (0, (5, 0.5))),\n",
    "    \n",
    "     ('loosely dashdotted',    (0, (3, 1, 1, 1))),   \n",
    "     ('dashdotted',            (0, (3, 0.5, 1, 0.5)))\n",
    "    ]\n",
    "    # CHANGE THIS IF YOU ADD A NEW TEACHER #\n",
    "    \n",
    "    linestyles = {}\n",
    "\n",
    "    df_columns = [ek + \"_\" + criterion for criterion in criterion_label.copy() for ek in ek_types.copy()]\n",
    "    df_indexes = list(set(labels.values()))\n",
    "    raw_results = {}\n",
    "    processed_results = pd.DataFrame(\n",
    "        index=df_indexes, \n",
    "        columns=df_columns)\n",
    "    statistical_test_results = pd.DataFrame(\n",
    "        index=df_indexes, \n",
    "        columns=df_columns)\n",
    "    \n",
    "    ### Get results ###\n",
    "    linestyle_iterrator = 0\n",
    "    for expe_id in models_saves:\n",
    "        if name_regex.match(expe_id):\n",
    "            current_label = labels[expe_id]\n",
    "            if list_of_teachers is not None and current_label not in list_of_teachers:\n",
    "                break\n",
    "                \n",
    "            if current_label not in raw_results:\n",
    "                raw_results[current_label] = {}\n",
    "                \n",
    "            if current_label not in linestyles and \"UPPER_BASELINE\" not in current_label:\n",
    "                linestyles[current_label] = linestyle_tuple[linestyle_iterrator][1]\n",
    "                linestyle_iterrator += 1\n",
    "            \n",
    "            # Get column prefix\n",
    "            if \"allow_expert_knowledge_no\" in expe_id:\n",
    "                prefix = \"no_\"\n",
    "            elif \"allow_expert_knowledge_minimal\" in expe_id:\n",
    "                prefix = \"low_\"\n",
    "            elif \"allow_expert_knowledge_maximal\" in expe_id:\n",
    "                prefix = \"high_\"\n",
    "            else:\n",
    "                raise Exception()\n",
    "                \n",
    "            # Get criteria\n",
    "            match = criteria_regex.match(expe_id)\n",
    "            criteria_id = match.group()[-1]\n",
    "            column = prefix + criterion_label[int(criteria_id) - 1]\n",
    "                \n",
    "            current_criteria_values = []\n",
    "            nb_seeds = len(models_saves[expe_id][\"data\"])\n",
    "            for seed in range(nb_seeds):\n",
    "                seed_values_array = models_saves[expe_id][\"data\"][seed][\"nb_mastered\"]\n",
    "                if timestep != -1 and len(seed_values_array) > timestep and not \"UPPER_BASELINE\" in current_label:\n",
    "                    seed_value = seed_values_array[timestep]\n",
    "                else:\n",
    "                    seed_value = seed_values_array[-1]\n",
    "                current_criteria_values.append(seed_value)\n",
    "            \n",
    "            raw_results[current_label][column] = current_criteria_values\n",
    "            \n",
    "            \n",
    "    ### Generate chart results ###\n",
    "    for teacher in raw_results:\n",
    "        for experiment in raw_results[teacher]:\n",
    "            if teacher == baseline_teacher:\n",
    "                processed_results.loc[teacher][experiment] = 1\n",
    "            else:\n",
    "                baseline_val = np.mean(raw_results[baseline_teacher][experiment])\n",
    "                current_val = np.mean(raw_results[teacher][experiment])\n",
    "                processed_results.loc[teacher][experiment] = current_val / baseline_val\n",
    "                \n",
    "                # Statistical tests\n",
    "                if not \"UPPER_BASELINE\" in teacher and current_val > baseline_val:\n",
    "                    ttest_result = get_simple_welch(raw_results[teacher][experiment],\n",
    "                                                    raw_results[baseline_teacher][experiment])\n",
    "                    if ttest_result[1] < p_value:\n",
    "                        statistical_test_results.loc[teacher][experiment] = True\n",
    "                    else:\n",
    "                        statistical_test_results.loc[teacher][experiment] = False\n",
    "       \n",
    "    ### Generate plot ### \n",
    "    N = len(processed_results.columns) / len(ek_types)\n",
    "    N = int(N)\n",
    "    \n",
    "    # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "    angles = [n / float(N) * 2 * math.pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    # Initialise the plot\n",
    "    nb_columns = 2\n",
    "    nb_rows = 2      \n",
    "    fig = plt.figure(constrained_layout=True, figsize=(80, 60))\n",
    "    gs = fig.add_gridspec(nb_rows, nb_columns)\n",
    "    fig.subplots_adjust(hspace=0.05)\n",
    "    for i in range(len(ek_types)):\n",
    "        current_ek_filter = ek_types[i] + \"_\"\n",
    "        if i == 0:\n",
    "            subplot_pos = gs[0, :]\n",
    "        else:\n",
    "            subplot_pos = gs[1, (i-1)%2]\n",
    "        ax = fig.add_subplot(subplot_pos, polar=True)\n",
    "        ax.patch.set_facecolor('white')\n",
    "        ax.title.set_text((ek_types[i] + \" expert knowledge\").capitalize())\n",
    "        ax.title.set_fontsize(80)\n",
    "        ax.tick_params(axis='both', which='major', pad=-50)\n",
    "\n",
    "        # Set labels\n",
    "        plt.xticks(angles[:-1], criterion_label, color='grey', size=65)\n",
    "        \n",
    "        for label, rad_angle in zip(ax.get_xticklabels(), angles):\n",
    "            statistical_results = statistical_test_results[current_ek_filter + label.get_text()]\n",
    "            xpos, ypos = (np.cos(rad_angle) + 1) / 2, (np.sin(rad_angle) + 1) / 2\n",
    "            ypos2 = ypos - 0.08\n",
    "            \n",
    "            angle = np.rad2deg(rad_angle)\n",
    "            if angle == 0:\n",
    "                label.set_horizontalalignment('left')\n",
    "                ypos2 = ypos - 0.1\n",
    "            elif angle == 180:\n",
    "                label.set_horizontalalignment('right')\n",
    "                ypos2 = ypos - 0.1\n",
    "            elif angle == 90 or angle == 270:\n",
    "                label.set_horizontalalignment('center')\n",
    "            elif 0 < angle < 90:\n",
    "                label.set_horizontalalignment('left')\n",
    "                label.set_verticalalignment('baseline')\n",
    "                ypos2 = ypos - 0.09\n",
    "            elif 90 < angle < 180:\n",
    "                label.set_horizontalalignment('right')\n",
    "                label.set_verticalalignment('baseline')\n",
    "                xpos -= 0.22\n",
    "            elif 180 < angle < 270:\n",
    "                label.set_horizontalalignment('right')\n",
    "                label.set_verticalalignment('center')\n",
    "                xpos -= 0.17\n",
    "            else:\n",
    "                label.set_horizontalalignment('left')\n",
    "                label.set_verticalalignment('center_baseline')\n",
    "            \n",
    "            pos_index = 0\n",
    "            for index, value in statistical_results.items():\n",
    "                if value is True:\n",
    "                    plt.text(xpos + pos_index, ypos2, r'$\\star$', \n",
    "                             size=120, color=per_model_colors[index],\n",
    "                             transform=ax.transAxes\n",
    "                    )\n",
    "                    pos_index += 0.04\n",
    "\n",
    "        # Set tick lines\n",
    "        for line in ax.xaxis.get_gridlines():\n",
    "            line.set_color('grey')\n",
    "            line.set_alpha(0.95)\n",
    "            line.set_linestyle(':')\n",
    "            line.set_linewidth(2)\n",
    "\n",
    "        for line in ax.yaxis.get_gridlines():\n",
    "            line.set_color('grey')\n",
    "            line.set_alpha(0.95)\n",
    "            line.set_linestyle(':')\n",
    "            line.set_linewidth(2)\n",
    "\n",
    "        # Change radar's background\n",
    "        max_val = max(processed_results.filter(like=current_ek_filter).max())\n",
    "        ticks = []\n",
    "        fill_values = np.linspace(0, 2*np.pi, 100)\n",
    "        for j in np.arange(0, max_val+tick_step, tick_step):\n",
    "            ticks.append(j)\n",
    "            ax.fill(fill_values, [j,] * 100, color='k', alpha=0.025)\n",
    "\n",
    "        # Draw ylabels\n",
    "        ax.set_rlabel_position(0)\n",
    "        plt.yticks(ticks, [str(t) for t in ticks], color='k', alpha=0.75, fontsize=55, ha=\"center\")\n",
    "        plt.ylim(0,max_val + tick_step)\n",
    "        ax.set_axisbelow(False)\n",
    "        \n",
    "        for index, row in processed_results.filter(like=current_ek_filter).iterrows():\n",
    "            values = list(row.values)\n",
    "            values += values[:1]\n",
    "            if index == \"UPPER_BASELINE\":\n",
    "                ax.plot(angles, values, \"P\", color='red', markersize=28)\n",
    "            else:\n",
    "                # Plot data\n",
    "                ax.plot(angles, values, linewidth=10, linestyle=linestyles[index], \n",
    "                        color=per_model_colors[index], \n",
    "                        label=index + (\" (baseline)\" if index == baseline_teacher else \"\"))\n",
    "                # Fill area\n",
    "                ax.fill(angles, values, 'b', alpha=0.065, color=per_model_colors[index])                  \n",
    "\n",
    "        if i == 0:\n",
    "            from matplotlib.lines import Line2D\n",
    "            handles = {}\n",
    "            legend = ax.legend(loc=(1.5,0.2), fontsize=65)\n",
    "            for legobj in legend.legendHandles:\n",
    "                current_legobj = copy.copy(legobj)\n",
    "                current_legobj.set_linewidth(15)\n",
    "                handles[current_legobj._label] = current_legobj\n",
    "                \n",
    "            del legend\n",
    "            # CHANGE THIS IF YOU ADD A NEW TEACHER #\n",
    "            ax.legend(handles=[\n",
    "                handles['Random (baseline)'],\n",
    "                handles['ALP-GMM'],\n",
    "                handles['Covar-GMM'],\n",
    "                handles['RIAC'],\n",
    "                handles['Self-Paced'],\n",
    "                Line2D([], [], linestyle='', label=\"\"),\n",
    "                Line2D([], [], linestyle='', label='$\\it{EK}$* $\\it{required}$:'),\n",
    "                handles['ADR'],\n",
    "                handles['GoalGAN'],\n",
    "                handles['Setter-Solver'],\n",
    "            ], loc=(1.5,0.1), fontsize=65)\n",
    "            # CHANGE THIS IF YOU ADD A NEW TEACHER #\n",
    "    \n",
    "    plt.savefig('../TeachMyAgent/graphics/{}.png'.format(figname), bbox_inches='tight', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stump Tracks bar plot comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAREFUL: IT NEEDS RENAME_LABELS SET TO TRUE AS WELL AS PER MODEL COLORS ACTIVATED\n",
    "def barplot_annotate_brackets(num1, num2, text, center, height, height_index, yerr=None, barh=.05):\n",
    "    \"\"\"\n",
    "        Add annotation brackets to plot.\n",
    " \n",
    "        :param num1: Index of first bar\n",
    "        :param num2: Index of second bar\n",
    "        :param text: Text to print above bracket\n",
    "        :param center: List of bars' x position\n",
    "        :param height: List of bars' y height\n",
    "        :param height_index: Distance between the highest bar (of the two selected) and the bracket\n",
    "        :param yerr: Errors of bars (default None)\n",
    "        :param barh: Bracket's vertical bars height\n",
    "    \"\"\"\n",
    "    ref_y = max(height)\n",
    "\n",
    "    lx, ly = center[num1], height[num1]\n",
    "    rx, ry = center[num2], height[num2]\n",
    "\n",
    "    if yerr:\n",
    "        ly += yerr[num1]\n",
    "        ry += yerr[num2]\n",
    "\n",
    "    ax_y0, ax_y1 = plt.gca().get_ylim()\n",
    "    barh *= (ax_y1 - ax_y0)\n",
    "\n",
    "    y = max(height) + height_index\n",
    "\n",
    "    barx = [lx, lx, rx, rx]\n",
    "    bary = [y, y+barh, y+barh, y]\n",
    "    mid = ((lx+rx)/2, y+barh)\n",
    "\n",
    "    plt.plot(barx, bary, c='black', linestyle='solid', linewidth=3)#(0, (1, 0.5)\n",
    "\n",
    "    kwargs = dict(ha='center', va='bottom')\n",
    "\n",
    "    plt.text(*mid, text, **kwargs)\n",
    "    \n",
    "def generate_comparison_bars(expe_template_name=\"*profiling_benchmark_stumps_*_criteria_(1|2|3|4|5)_allow_expert_knowledge_(no|minimal|maximal)$\", \n",
    "                             list_of_teachers=None, timestep=-1, p_value=0.05):\n",
    "    \"\"\"\n",
    "        Bar plots of Stump Tracks experiments (i.e. figure 8 of our paper).\n",
    " \n",
    "        :param expe_template_name: Regex filtering experiments to show (THIS CAN BE LEFT TO ITS DEFAULT VALUE)\n",
    "        :param list_of_teachers: Filter teachers to show by their label (if None all teachers are plotted)\n",
    "        :param timestep: Plot results at a certain timestep of training (-1 means end of training)\n",
    "        :param p_value: P-value threshold to reject null hypothesis\n",
    "    \"\"\"\n",
    "    anything_token = \"*\"\n",
    "    anything_pattern = \"[a-zA-z0-9\\-.]*\"\n",
    "    experiment_name_regex_pattern = expe_template_name.replace(anything_token, anything_pattern)\n",
    "    name_regex = re.compile(experiment_name_regex_pattern)\n",
    "    criteria_regex = re.compile(anything_pattern + \"_criteria_[0-9]\")\n",
    "    \n",
    "    figname = \"benchmark_bars{}\".format('_' + '_'.join(list_of_teachers) if list_of_teachers is not None else '')\n",
    "    if timestep != -1:\n",
    "        figname += \"_timestep-\" + str(timestep)\n",
    "    \n",
    "    criterion_label = [\n",
    "        \"Mostly\\n unfeasible\\n task space\",\n",
    "        \"Mostly\\n trivial\\n task space\",\n",
    "        \"Student\\n that\\n forgets\",\n",
    "        \"Rugged\\n difficulty\",\n",
    "        \"Variety\\n of\\n students\"\n",
    "    ]\n",
    "    \n",
    "    ek_types = [\"no\", \"low\", \"high\"]\n",
    "\n",
    "    raw_results = {}\n",
    "    \n",
    "    for ek in ek_types:\n",
    "        raw_results[ek] = {}\n",
    "        for criterion in criterion_label:\n",
    "            raw_results[ek][criterion] = {}\n",
    "    \n",
    "    ### Get results ###\n",
    "    linestyle_iterrator = 0\n",
    "    for expe_id in models_saves:\n",
    "        if name_regex.match(expe_id):\n",
    "            current_label = labels[expe_id]\n",
    "            if list_of_teachers is not None and current_label not in list_of_teachers:\n",
    "                break\n",
    "            \n",
    "            # Get column prefix\n",
    "            if \"allow_expert_knowledge_no\" in expe_id:\n",
    "                results_index = \"no\"\n",
    "            elif \"allow_expert_knowledge_minimal\" in expe_id:\n",
    "                results_index = \"low\"\n",
    "            elif \"allow_expert_knowledge_maximal\" in expe_id:\n",
    "                results_index = \"high\"\n",
    "            else:\n",
    "                raise Exception()                   \n",
    "            # Get criteria\n",
    "            match = criteria_regex.match(expe_id)\n",
    "            criteria_id = match.group()[-1]\n",
    "            current_criterion = criterion_label[int(criteria_id) - 1]\n",
    "                \n",
    "            current_criteria_values = []\n",
    "            nb_seeds = len(models_saves[expe_id][\"data\"])\n",
    "            for seed in range(nb_seeds):\n",
    "                seed_values_array = models_saves[expe_id][\"data\"][seed][\"nb_mastered\"]\n",
    "                if timestep != -1 and len(seed_values_array) > timestep:\n",
    "                    seed_value = seed_values_array[timestep]\n",
    "                else:\n",
    "                    seed_value = seed_values_array[-1]\n",
    "                current_criteria_values.append(seed_value)\n",
    "            \n",
    "            mean_result = np.mean(current_criteria_values)\n",
    "            std_result = np.std(current_criteria_values)\n",
    "            raw_results[results_index][current_criterion][current_label] = {\n",
    "                \"mean\": mean_result,\n",
    "                \"std\": std_result,\n",
    "                \"seeds\": current_criteria_values\n",
    "            }\n",
    "\n",
    "                \n",
    "    ### Generate plot ### \n",
    "    # Initialise the plot\n",
    "    nb_columns = len(ek_types) + 1\n",
    "    nb_rows = len(criterion_label) + 1\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(100, 150))\n",
    "    widths = [0.1 if i == 0 else 1 for i in range(nb_columns)]\n",
    "    heights = [0.1 if i == 0 else 1 for i in range(nb_rows)]\n",
    "    gs = fig.add_gridspec(nb_rows, nb_columns, width_ratios=widths, height_ratios=heights)\n",
    "    fig.subplots_adjust(wspace=0.05)\n",
    "    \n",
    "    for i in range(nb_columns):\n",
    "        for j in range(nb_rows):\n",
    "            ax = fig.add_subplot(gs[j, i])\n",
    "            if i == 0 or j == 0:\n",
    "                ax.set_axis_off()\n",
    "                if i + j > 0:\n",
    "                    if i == 0:\n",
    "                        ax.text(0.5, 0.5, criterion_label[j-1], ha=\"center\", va=\"center\", fontsize=50)\n",
    "                    else:\n",
    "                        ax.text(0.5, 0.5, (ek_types[i-1] + \" expert knowledge\").capitalize(), ha=\"center\", va=\"center\", fontsize=50)\n",
    "                    \n",
    "            else:\n",
    "                current_ek_type = ek_types[i-1]\n",
    "                current_criterion = criterion_label[j-1]\n",
    "                current_expe = raw_results[current_ek_type][current_criterion]\n",
    "                means = []\n",
    "                stds = []\n",
    "                current_labels = []\n",
    "                current_colors = []\n",
    "\n",
    "                current_indexes = np.arange(len((current_expe.keys())))\n",
    "                is_diff_significant = []\n",
    "                t1 = 0\n",
    "                for teacher_1 in current_expe.keys():\n",
    "                    means.append(current_expe[teacher_1][\"mean\"])\n",
    "                    stds.append(current_expe[teacher_1][\"std\"])\n",
    "                    current_labels.append(teacher_1)\n",
    "                    current_colors.append(per_model_colors[teacher_1])\n",
    "                    t2 = 0\n",
    "                    for teacher_2 in current_expe.keys():\n",
    "                        if t2 > t1:\n",
    "                            if teacher_1 != teacher_2:\n",
    "                                ttest_result = get_simple_welch(current_expe[teacher_1][\"seeds\"], \n",
    "                                                                current_expe[teacher_2][\"seeds\"])\n",
    "                                if ttest_result[1] < p_value:\n",
    "                                    is_diff_significant.append((t1, t2))\n",
    "                        t2 += 1\n",
    "                    t1 += 1\n",
    "\n",
    "                ax.p1 = plt.bar(current_indexes, means, color=current_colors)\n",
    "#                 ax.errs = plt.errorbar(current_indexes, means, yerr=stds)\n",
    "\n",
    "                k = 0\n",
    "                for significant_diff in is_diff_significant:\n",
    "                    barplot_annotate_brackets(significant_diff[0], significant_diff[1], \n",
    "                                              \"\", current_indexes, means, k, barh=0.01)\n",
    "                    k += 2.2\n",
    "                plt.xticks(current_indexes, current_labels, fontsize=30)\n",
    "                plt.ylim(ymax=100)\n",
    "    \n",
    "    plt.savefig('../TeachMyAgent/graphics/{}.png'.format(figname), bbox_inches='tight', dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the function calls below to plot your experiments. You can use regex-like patterns with some modifications:\n",
    "- `*` means anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_labels = copy.copy(labels) # Prepare new labels for some plots (e.g 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## % Nb mastered Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"benchmark_parkour_(ADR|ALP-GMM|Covar-GMM|RIAC|Random|Setter-Solver|Self-Paced|GoalGAN)$\", leg_size=36, leg_loc=(0,0.39), y_max=50, plot_type=\"shade_se\", allow_different_sizes=True, x_max=20.2, welch=Statistical_tests_enum.MULTI_STEP, welch_baseline=\"benchmark_parkour_Random\", welch_step_freq=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALP-GMM\n",
    "new_labels[\"09-12_profiling_benchmark_stumps_ALP-GMM_criteria_3_allow_expert_knowledge_no\"] = \"No expert knowledge\"\n",
    "new_labels[\"09-12_profiling_benchmark_stumps_ALP-GMM_criteria_3_allow_expert_knowledge_minimal\"] = \"Low expert knowledge\"\n",
    "new_labels[\"09-12_profiling_benchmark_stumps_ALP-GMM_criteria_3_allow_expert_knowledge_maximal\"] = \"High expert knowledge\"\n",
    "# SPDL\n",
    "new_labels[\"11-12_profiling_benchmark_stumps_Self-Paced_criteria_3_allow_expert_knowledge_no\"] = \"No expert knowledge\"\n",
    "new_labels[\"11-12_profiling_benchmark_stumps_Self-Paced_criteria_3_allow_expert_knowledge_minimal\"] = \"Low expert knowledge\"\n",
    "new_labels[\"11-12_profiling_benchmark_stumps_Self-Paced_criteria_3_allow_expert_knowledge_maximal\"] = \"High expert knowledge\"\n",
    "# ADR\n",
    "new_labels[\"09-12_profiling_benchmark_stumps_ADR_criteria_3_allow_expert_knowledge_minimal\"] = \"Low expert knowledge\"\n",
    "new_labels[\"09-12_profiling_benchmark_stumps_ADR_criteria_3_allow_expert_knowledge_maximal\"] = \"High expert knowledge\"\n",
    "# GoalGAN\n",
    "new_labels[\"14-12_profiling_benchmark_stumps_GoalGAN_criteria_3_allow_expert_knowledge_minimal\"] = \"Low expert knowledge\"\n",
    "new_labels[\"14-12_profiling_benchmark_stumps_GoalGAN_criteria_3_allow_expert_knowledge_maximal\"] = \"High expert knowledge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_ALP-GMM_criteria_3*_(maximal|no|minimal)\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_max=60, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_Self-Paced_criteria_3*_(maximal|no|minimal)\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_max=60, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_ADR_criteria_3*_(maximal|minimal)\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_max=60, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_GoalGAN_criteria_3*_(maximal|minimal)\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_max=60, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALP-GMM - Criteria 1\n",
    "new_labels[\"07-12_profiling_benchmark_stumps_ALP-GMM_criteria_1_allow_expert_knowledge_no\"] = \"No expert knowledge\"\n",
    "new_labels[\"07-12_profiling_benchmark_stumps_ALP-GMM_criteria_1_allow_expert_knowledge_minimal\"] = \"Low expert knowledge\"\n",
    "new_labels[\"07-12_profiling_benchmark_stumps_ALP-GMM_criteria_1_allow_expert_knowledge_maximal\"] = \"High expert knowledge\"\n",
    "# ALP-GMM - Criteria 3\n",
    "new_labels[\"09-12_profiling_benchmark_stumps_ALP-GMM_criteria_3_allow_expert_knowledge_no\"] = \"No expert knowledge\"\n",
    "new_labels[\"09-12_profiling_benchmark_stumps_ALP-GMM_criteria_3_allow_expert_knowledge_minimal\"] = \"Low expert knowledge\"\n",
    "new_labels[\"09-12_profiling_benchmark_stumps_ALP-GMM_criteria_3_allow_expert_knowledge_maximal\"] = \"High expert knowledge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_ALP-GMM_criteria_1*_(maximal|minimal)\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_max=65, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.MULTI_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_ALP-GMM_criteria_1*_(maximal|no)\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_max=65, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.MULTI_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_ALP-GMM_criteria_3*_(maximal|minimal)\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_max=65, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.MULTI_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_ALP-GMM_criteria_3*_(maximal|no)\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_max=65, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.MULTI_STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADR - Criteria 1\n",
    "new_labels[\"09-12_profiling_benchmark_stumps_ADR_criteria_1_allow_expert_knowledge_minimal\"] = \"Low expert knowledge\"\n",
    "new_labels[\"09-12_profiling_benchmark_stumps_ADR_criteria_1_allow_expert_knowledge_maximal\"] = \"High expert knowledge\"\n",
    "# ADR - Criteria 2\n",
    "new_labels[\"09-12_profiling_benchmark_stumps_ADR_criteria_2_allow_expert_knowledge_minimal\"] = \"Low expert knowledge\"\n",
    "new_labels[\"09-12_profiling_benchmark_stumps_ADR_criteria_2_allow_expert_knowledge_maximal\"] = \"High expert knowledge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_ADR_criteria_1*_(maximal|minimal)\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_max=65, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.MULTI_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_ADR_criteria_2*_(maximal|minimal)\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_max=65, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.MULTI_STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADR - Criteria 4\n",
    "new_labels[\"18-12_profiling_benchmark_stumps_ADR_criteria_4_allow_expert_knowledge_minimal\"] = \"Low expert knowledge\"\n",
    "new_labels[\"18-12_profiling_benchmark_stumps_ADR_criteria_4_allow_expert_knowledge_maximal\"] = \"High expert knowledge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_ADR_criteria_4*_(maximal|minimal)\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_max=65, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.MULTI_STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*benchmark_parkour_(ADR|ALP-GMM|Covar-GMM|RIAC|Random|Setter-Solver|Self-Paced|GoalGAN)_walker_type_old_classic_bipedal\",leg_size=30, y_max=63, plot_type=\"shade_se\", allow_different_sizes=True, x_max=20.2, welch=Statistical_tests_enum.MULTI_STEP, welch_baseline=\"16-12_benchmark_parkour_Random_walker_type_old_classic_bipedal\", welch_step_freq=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*benchmark_parkour_(ADR|ALP-GMM|Covar-GMM|RIAC|Random|Setter-Solver|Self-Paced|GoalGAN)_walker_type_fish\", leg_size=30, y_max=60, plot_type=\"shade_se\", allow_different_sizes=True, x_max=20.2, welch=Statistical_tests_enum.MULTI_STEP, welch_baseline=\"16-12_benchmark_parkour_Random_walker_type_fish\", welch_step_freq=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*benchmark_parkour_(ADR|ALP-GMM|Covar-GMM|RIAC|Random|Setter-Solver|Self-Paced|GoalGAN)_walker_type_climbing_profile_chimpanzee\",leg_size=30, y_max=4, plot_type=\"shade_se\", allow_different_sizes=True, x_max=20.2, welch=Statistical_tests_enum.MULTI_STEP, welch_baseline=\"16-12_benchmark_parkour_Random_walker_type_climbing_profile_chimpanzee\", welch_step_freq=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_ALP-GMM_criteria_3*_(maximal|no|minimal)\", metric=\"training return\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_min=-200, y_max=310, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_Self-Paced_criteria_3*_(maximal|no|minimal)\", metric=\"training return\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_min=-200, y_max=310, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_ADR_criteria_3*_(maximal|minimal)\", metric=\"training return\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_min=-200, y_max=310, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_GoalGAN_criteria_3*_(maximal|minimal)\", metric=\"training return\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_min=-200, y_max=310, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_ALP-GMM_criteria_3*_(maximal|no|minimal)\", metric=\"training return\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_min=-200, y_max=310, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_ALP-GMM_criteria_1*_(maximal|no|minimal)\", metric=\"training return\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_min=-200, y_max=310, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_ADR_criteria_1*_(maximal|minimal)\", metric=\"training return\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_min=-200, y_max=310, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_ADR_criteria_2*_(maximal|minimal)\", metric=\"training return\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_min=-200, y_max=310, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_ADR_criteria_4*_(maximal|minimal)\", metric=\"training return\", plot_type=\"shade_se\", allow_different_sizes=True, x_max=20, y_min=-200, y_max=310, new_labels=new_labels, leg_size=30, welch=Statistical_tests_enum.NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*profiling_benchmark_stumps_Covar-GMM_criteria_5*_no_\", x_max=20, plot_type=\"shade_se\", metric=\"evaluation return\", allow_different_sizes=True, y_min=-300, y_max=310, welch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(\"*subset_parkour_climbing_easy_parkour_1*Random\", leg_size=30, x_max=10, welch=False, allow_different_sizes=True, plot_type=\"shade_se\", metric=\"training return\", y_min=-200, y_max=310)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parkour (figure 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_comparisons(\"benchmark_parkour_(ADR|ALP-GMM|Covar-GMM|RIAC|Random|Setter-Solver|Self-Paced|GoalGAN)$\", metric=\"nb_mastered\", x_max=20.5, y_min=0, y_max=45, allow_different_sizes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stump Tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radar chart (figure 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_profile_chart(baseline_teacher=\"Random\", tick_step=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Radar chart 10 millions steps (figure 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_profile_chart(baseline_teacher=\"Random\", tick_step=1, timestep=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bar plot (figure 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_comparison_bars()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
